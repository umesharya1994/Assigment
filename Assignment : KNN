Bagging & Boosting KNN & Stacking | Assignment Answers
Question 1: What is the fundamental idea behind ensemble techniques? How does bagging differ from boosting in terms of approach and objective?
Answer:

Fundamental Idea Behind Ensemble Techniques:

Ensemble methods combine multiple machine learning models to produce a single, more powerful predictor. The core principle is that a group of "weak learners" can come together to form a "strong learner" that performs better than any individual model. This is analogous to the "wisdom of the crowd" concept â€“ multiple diverse opinions often lead to better decisions than a single expert.

Key Intuition:

Individual models may make different errors

By combining them, errors can cancel out

The ensemble captures a broader perspective of the data

Comparison: Bagging vs. Boosting

Aspect	Bagging (Bootstrap Aggregating)	Boosting
Approach	Parallel training of independent models	Sequential training of dependent models
Objective	Reduce variance (overfitting)	Reduce bias (underfitting)
Data Sampling	Bootstrap sampling (random samples with replacement)	All data used, but weights adjusted
Model Weight	Equal voting for all models	Weighted voting based on performance
Focus	Each model trained independently on different data subsets	Each new model focuses on errors of previous models
Example	Random Forest	AdaBoost, Gradient Boosting, XGBoost
Visual Representation:

text
Bagging (Parallel):
    Data
      â†“
    Bootstrap
   â”Œâ”€â”€â”€â”¬â”€â”€â”€â”¬â”€â”€â”€â”
   â†“   â†“   â†“   â†“
  Model Model Model Model
   â†“   â†“   â†“   â†“
    Aggregation (Voting/Averaging)
      â†“
    Final Prediction

Boosting (Sequential):
    Data â†’ Model 1 â†’ Errors â†’ Model 2 â†’ Errors â†’ Model 3 â†’ ...
             â†‘                    â†‘                    â†‘
         Weighted           Weighted             Weighted
         Data                Data                  Data
      â†“
    Weighted Combination
      â†“
    Final Prediction
Mathematical Insight:

Bagging: Reduces variance by averaging multiple models: Var(ensemble) = Var(model) / n (if models are independent)

Boosting: Reduces bias by iteratively correcting errors, allowing the ensemble to fit complex patterns

Question 2: Explain how the Random Forest Classifier reduces overfitting compared to a single decision tree. Mention the role of two key hyperparameters in this process.
Answer:

How Random Forest Reduces Overfitting:

A single decision tree tends to overfit because it can grow deep enough to memorize the training data, including noise. Random Forest combats this through two forms of randomization that create diverse, uncorrelated trees whose errors cancel out when averaged.

Key Mechanisms:

Bootstrap Sampling (Bagging): Each tree is trained on a different random sample of the data (with replacement). This means each tree sees slightly different data, making them diverse.

Random Feature Selection: At each split, only a random subset of features is considered. This prevents strong features from dominating all trees and forces trees to use different patterns.

Visual Analogy:

Single Tree: One expert who might have biases and blind spots

Random Forest: A committee of diverse experts who each look at different aspects of the problem

Key Hyperparameters and Their Roles:

Hyperparameter	Role in Reducing Overfitting
n_estimators	Number of trees in the forest. More trees â†’ more averaging â†’ lower variance. However, diminishing returns after a point.
max_features	Size of the random feature subset considered at each split. Smaller values â†’ more diverse trees â†’ less correlation â†’ better overfitting protection.
max_depth	Maximum depth of each tree. Shallower trees are simpler and less likely to overfit.
min_samples_split	Minimum samples required to split a node. Higher values prevent trees from learning noise.
min_samples_leaf	Minimum samples required in a leaf node. Higher values create smoother boundaries.
Impact of Key Hyperparameters:

text
Effect of max_features:
    max_features = n_features (all features) â†’ Trees are similar â†’ less diversity
    max_features = sqrt(n_features) (typical default) â†’ Good balance
    max_features = 1 or 2 â†’ Very diverse trees, might underfit

Effect of n_estimators:
    Error
      â†‘
      â”‚    Training Error
      â”‚    â•±
      â”‚   â•±    Test Error
      â”‚  â•±    â•±
      â”‚ â•±    â•±
      â”‚â•±    â•±
      â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â†’ n_estimators
           More trees â†’ test error stabilizes, doesn't increase
Why It Works:

Diversity: Different trees make different mistakes

Averaging: When averaging predictions, individual errors cancel out

Law of Large Numbers: As number of trees increases, ensemble converges to expected value

Comparison:

Model	Variance	Bias	Interpretability
Single Decision Tree	High	Low	High
Random Forest	Low	Low (slightly higher than single tree)	Medium (feature importance)
Question 3: What is Stacking in ensemble learning? How does it differ from traditional bagging/boosting methods? Provide a simple example use case.
Answer:

Stacking (Stacked Generalization) is an ensemble learning technique that combines multiple base models (level-0 models) using a meta-model (level-1 model) to make final predictions. The meta-model learns how to best combine the predictions of the base models.

Architecture:

text
Stacking Architecture:

                     â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”
                     â”‚   Training  â”‚
                     â”‚    Data     â”‚
                     â””â”€â”€â”€â”€â”€â”€â”¬â”€â”€â”€â”€â”€â”€â”˜
          â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¼â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”
          â†“                 â†“                 â†“
    â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”     â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”     â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”
    â”‚ Model A  â”‚     â”‚ Model B  â”‚     â”‚ Model C  â”‚
    â”‚ (Base)   â”‚     â”‚ (Base)   â”‚     â”‚ (Base)   â”‚
    â””â”€â”€â”€â”€â”¬â”€â”€â”€â”€â”€â”˜     â””â”€â”€â”€â”€â”¬â”€â”€â”€â”€â”€â”˜     â””â”€â”€â”€â”€â”¬â”€â”€â”€â”€â”€â”˜
         â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¼â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜
                          â†“
                   â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”
                   â”‚ Predictions â”‚
                   â”‚  as Featuresâ”‚
                   â””â”€â”€â”€â”€â”€â”€â”¬â”€â”€â”€â”€â”€â”€â”˜
                          â†“
                   â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”
                   â”‚ Meta-Model  â”‚
                   â”‚  (Level-1)  â”‚
                   â””â”€â”€â”€â”€â”€â”€â”¬â”€â”€â”€â”€â”€â”€â”˜
                          â†“
                   â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”
                   â”‚   Final     â”‚
                   â”‚ Prediction  â”‚
                   â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜
Key Differences from Bagging/Boosting:

Aspect	Bagging	Boosting	Stacking
Model Training	Parallel	Sequential	Parallel (base), then meta
Base Models	Same type (usually)	Same type	Can be different types
Combination Method	Voting/Averaging	Weighted sum	Learned by meta-model
Diversity Source	Data sampling	Error weighting	Different algorithms
Complexity	Low-Medium	Medium	High
Simple Example Use Case:

Scenario: Predicting house prices with heterogeneous data types.

Base Models (Level-0):

Linear Regression: Good for capturing linear trends in numerical features (square footage, lot size)

Decision Tree: Good for capturing non-linear interactions and categorical features (neighborhood, house style)

KNN: Good for finding similar houses based on location and features

Meta-Model (Level-1):

Logistic Regression (for classification) or Ridge Regression (for regression)

Process:

Split training data into two parts: Train A and Train B

Train base models on Train A

Generate predictions for Train B using trained base models

Create new feature set where each feature is a prediction from a base model

Train meta-model on these predictions (using Train B's true labels)

For new data: Get predictions from all base models, feed to meta-model for final prediction

Advantages of Stacking:

Advantage	Explanation
Leverages diverse models	Can combine strengths of different algorithms
Learns optimal combination	Meta-model discovers how to best blend predictions
Often outperforms single models	State-of-the-art in many competitions
Flexible architecture	Any models can be used at any level
Disadvantages:

Disadvantage	Explanation
Computationally expensive	Requires training multiple models
Risk of overfitting	Need careful cross-validation design
Complexity	Harder to interpret and debug
More hyperparameters	Each base model + meta-model needs tuning
Winning Strategy: Stacking is a common technique in Kaggle competitions, often combining 5-10 diverse models with a simple linear meta-model.

Question 4: What is the OOB Score in Random Forest, and why is it useful? How does it help in model evaluation without a separate validation set?
Answer:

OOB (Out-of-Bag) Score is an internal validation metric in Random Forest that estimates the model's performance using samples that were not included in the bootstrap sample for each tree.

How OOB Works:

Bootstrap Sampling: For each tree, about 63% of the original data is randomly selected (with replacement) for training. The remaining 37% are "out-of-bag" samples.

Prediction: For each data point, we collect predictions only from trees where that point was OOB (not used in training).

Aggregation: The OOB predictions are combined (by voting or averaging) to get a final OOB prediction for each sample.

OOB Score: Calculate accuracy (classification) or RÂ² (regression) by comparing OOB predictions with true values.

Visual Representation:

text
Dataset: [1, 2, 3, 4, 5, 6, 7, 8, 9, 10]

Tree 1 Bootstrap Sample (with replacement): [2, 5, 5, 7, 9, 10]
OOB for Tree 1: [1, 3, 4, 6, 8]

Tree 2 Bootstrap Sample: [1, 3, 4, 6, 8, 8, 10]
OOB for Tree 2: [2, 5, 7, 9]

For sample 5: OOB predictions from Tree 2 (since it's OOB there)
For sample 8: OOB predictions from Tree 1
Why OOB Score is Useful:

Reason	Explanation
No separate validation set needed	Uses data efficiently, all samples for training and validation
Unbiased estimate	Similar to cross-validation but computationally cheaper
Built-in during training	Calculated automatically without extra work
Good approximation of test error	Correlates well with true test performance
Comparison with Cross-Validation:

Aspect	OOB Score	K-Fold Cross-Validation
Computation	Free (during training)	Requires retraining K times
Bias	Slightly optimistic	Nearly unbiased
Variance	Lower	Higher
Sample usage	Each point used in ~63% of trees	Each point used in K-1 folds
Speed	Fast	Slow (K times training)
Mathematical Insight:

For each tree, the probability a sample is not selected in bootstrap is:
P(not selected) = (1 - 1/n)^n â‰ˆ 1/e â‰ˆ 0.368

So each sample is OOB for about 37% of trees, providing enough predictions for reliable estimation.

How to Access OOB Score in scikit-learn:

python
from sklearn.ensemble import RandomForestClassifier

# Enable oob_score
rf = RandomForestClassifier(n_estimators=100, oob_score=True, random_state=42)
rf.fit(X_train, y_train)

# Get OOB score
print(f"OOB Score: {rf.oob_score_:.4f}")

# Compare with validation score
print(f"Validation Score: {rf.score(X_val, y_val):.4f}")
Practical Benefits:

Hyperparameter Tuning: Use OOB score to tune parameters without a validation set

Feature Selection: Compare OOB scores with different feature sets

Early Stopping: Monitor OOB error during training (if implementing manually)

Model Comparison: Quick, unbiased comparison between different Random Forest configurations

Limitations:

Limitation	Explanation
Only for bagged ensembles	Not applicable to other models
Slightly optimistic	Especially for small datasets
Not perfect for very small n_estimators	Need enough trees for stable estimates
Classification only	For regression, OOB RÂ² is available but less common
Example:

python
# Without OOB (needs validation set)
X_train, X_val, y_train, y_val = train_test_split(X, y, test_size=0.2)
rf.fit(X_train, y_train)
val_score = rf.score(X_val, y_val)  # Uses 20% of data just for validation

# With OOB (uses all data for training)
rf_oob = RandomForestClassifier(oob_score=True)
rf_oob.fit(X, y)
oob_score = rf_oob.oob_score_  # No data wasted!
OOB Score is particularly valuable when data is limited, as it allows you to use all samples for training while still getting an unbiased performance estimate.

Question 5: Compare AdaBoost and Gradient Boosting in terms of how they handle errors from weak learners, weight adjustment mechanism, and typical use cases.
Answer:

AdaBoost (Adaptive Boosting) and Gradient Boosting are both boosting algorithms that combine weak learners sequentially, but they differ fundamentally in how they identify and correct errors.

Comprehensive Comparison:

Aspect	AdaBoost	Gradient Boosting
Year Introduced	1995 (Freund & Schapire)	1999 (Jerome Friedman)
Error Handling	Focus on misclassified samples	Focus on residual errors
Weight Adjustment	Sample weights updated	New model fits residuals
Loss Function	Exponential loss	Any differentiable loss function
Weak Learners	Usually stumps (depth=1)	Usually shallow trees (depth=3-8)
Learning Rate	Implicit (weights determine contribution)	Explicit shrinkage parameter
Detailed Comparison:

1. How They Handle Errors
AdaBoost:

Increases weights of misclassified samples

Decreases weights of correctly classified samples

Each new learner focuses on samples with highest weights

Intuition: "Pay more attention to hard examples"

text
AdaBoost Iteration:
Start: [â—‹ â—‹ â—‹ â—‹ â—‹] (equal weights)
Step 1: Model focuses on all â†’ errors on â— and â—
       [â—‹ â—‹ â—‹ â—‹ â—‹] â†’ weights: [1, 1, 1, 1, 1]
Step 2: Increase weights of â— and â—
       [â—‹ â—‹ â—‹ â—‹ â—‹] â†’ weights: [1, 1, 2, 2, 1]
Step 3: New model focuses on heavy â— and â—
Gradient Boosting:

Computes residual errors (difference between prediction and true value)

Fits new model to predict these residuals

Adds the new model (scaled by learning rate) to the ensemble

Intuition: "Take a step in the direction that reduces error"

text
Gradient Boosting Iteration:
Step 1: Initial model Fâ‚€(x) = mean(y)
Step 2: Compute residuals: râ‚ = y - Fâ‚€(x)
Step 3: Train hâ‚(x) to predict râ‚
Step 4: Update: Fâ‚(x) = Fâ‚€(x) + Î·Â·hâ‚(x)
Step 5: Repeat with new residuals
2. Weight Adjustment Mechanism
Aspect	AdaBoost	Gradient Boosting
What is Weighted?	Training samples	Residuals (implicitly)
How Updated?	Î± = Â½ ln((1-Îµ)/Îµ) where Îµ is weighted error	New model fits current residuals
Learner Contribution	Î± Ã— h(x) where Î± depends on accuracy	Î· Ã— h(x) where Î· is learning rate
Interpretation	Exponential weighting of samples	Gradient descent in function space
Mathematical Formulation:

AdaBoost:

text
Initialize weights: wáµ¢ = 1/n
For m = 1 to M:
    Fit classifier hâ‚˜(x) using weights wáµ¢
    Compute weighted error: Îµâ‚˜ = Î£ wáµ¢Â·I(yáµ¢ â‰  hâ‚˜(xáµ¢))
    Compute learner weight: Î±â‚˜ = Â½ ln((1-Îµâ‚˜)/Îµâ‚˜)
    Update weights: wáµ¢ â† wáµ¢Â·exp(Î±â‚˜Â·I(yáµ¢ â‰  hâ‚˜(xáµ¢)))
    Normalize weights
Final: H(x) = sign(Î£ Î±â‚˜Â·hâ‚˜(x))
Gradient Boosting:

text
Initialize Fâ‚€(x) = argmin_Î³ Î£ L(yáµ¢, Î³)
For m = 1 to M:
    Compute residuals: ráµ¢â‚˜ = -[âˆ‚L(yáµ¢, F(xáµ¢))/âˆ‚F(xáµ¢)] at F=Fâ‚˜â‚‹â‚
    Fit hâ‚˜(x) to residuals ráµ¢â‚˜
    Find optimal step size Ïâ‚˜ = argmin_Ï Î£ L(yáµ¢, Fâ‚˜â‚‹â‚(xáµ¢) + ÏÂ·hâ‚˜(xáµ¢))
    Update: Fâ‚˜(x) = Fâ‚˜â‚‹â‚(x) + Î·Â·Ïâ‚˜Â·hâ‚˜(x)
Final: F(x) = Fâ‚€(x) + Î£ Î·Â·Ïâ‚˜Â·hâ‚˜(x)
3. Typical Use Cases
Scenario	AdaBoost	Gradient Boosting
Binary Classification	âœ“ Excellent	âœ“ Excellent
Multi-class Classification	âœ“ Good (with modifications)	âœ“ Excellent
Regression	âœ— Not designed for it	âœ“ Excellent
Noisy Data	âœ— Sensitive to outliers	âœ“ Robust (with Huber loss)
High-dimensional sparse data	âœ“ Works well	âœ“ Works well
When interpretability matters	âœ“ Feature importance available	âœ“ Feature importance available
Small datasets	âœ“ Good	âš ï¸ Can overfit
Large datasets	âœ“ Scales well	âš ï¸ Can be slow
Practical Strengths and Weaknesses:

AdaBoost:

âœ… Simple, elegant algorithm

âœ… Works well with weak learners (stumps)

âœ… Theoretically well-understood

âŒ Sensitive to noisy data and outliers

âŒ Exponential loss is not robust

Gradient Boosting:

âœ… Highly flexible (any differentiable loss)

âœ… Usually achieves state-of-the-art performance

âœ… Handles mixed data types well

âœ… Built-in regularization options

âŒ More hyperparameters to tune

âŒ Can overfit if not carefully regularized

Modern Implementations:

Algorithm	Based On	Key Features
AdaBoost	AdaBoost	Original boosting algorithm
Gradient Boosting	Gradient Boosting	scikit-learn's GradientBoosting
XGBoost	Gradient Boosting	Regularization, parallel processing
LightGBM	Gradient Boosting	Leaf-wise growth, categorical support
CatBoost	Gradient Boosting	Categorical features, symmetric trees
Which to Choose?

Choose AdaBoost when: You have clean data, want simplicity, or need a quick baseline

Choose Gradient Boosting when: You need state-of-the-art performance, have mixed data types, or want flexibility in loss functions

Question 6: Why does CatBoost perform well on categorical features without requiring extensive preprocessing? Briefly explain its handling of categorical variables.
Answer:

CatBoost (Categorical Boosting) is a gradient boosting library developed by Yandex that excels at handling categorical features automatically, without requiring manual preprocessing like one-hot encoding.

Why CatBoost Performs Well on Categorical Features:

Built-in Categorical Handling: Native support for categorical features without preprocessing

Ordered Target Statistics: Prevents target leakage and overfitting

Feature Combinations: Automatically creates meaningful feature interactions

Symmetric Trees: Uses oblivious trees that treat categorical splits efficiently

Key Innovation: Ordered Target Statistics

CatBoost's core innovation is how it converts categorical features to numerical values while avoiding target leakage.

Problem with Traditional Target Encoding:

python
# Naive target encoding (leaks information!)
mean_encoded = df.groupby('category')['target'].mean()
# Uses target to encode â†’ data leakage!
CatBoost's Solution - Ordered Target Statistics:

For each sample, CatBoost calculates the target statistic using only previous samples in a random permutation:

text
Random Permutation: [sample_3, sample_1, sample_5, sample_2, sample_4]

For sample_5 (category='A'):
    Look at previous samples in permutation: sample_3, sample_1
    Calculate: (count_target_1_in_previous + prior) / (count_previous + 1)
How It Works:

Random Permutations: Generate multiple random orders of the dataset

Online Calculation: For each sample, use only previous samples to compute statistics

Prior Term: Add a prior value to smooth estimates, especially for rare categories

Averaging: Average results across multiple permutations for stability

Handling Categorical Features in CatBoost:

Feature Type	CatBoost Approach	Traditional Approach
Low-cardinality	Ordered target statistics	One-hot encoding
High-cardinality	Ordered target statistics with regularization	Target encoding (risky)
Text features	Can be treated as categorical	TF-IDF, word embeddings
Feature combinations	Automatically created	Manual feature engineering
Advantages of CatBoost's Approach:

Advantage	Explanation
No preprocessing needed	Just pass categorical column indices
Handles high cardinality	Works even with thousands of categories
Prevents target leakage	Ordered calculation ensures no future data used
Preserves information	Unlike one-hot, doesn't explode feature space
Feature combinations	Automatically captures interactions between categoricals
Example Usage:

python
from catboost import CatBoostClassifier
import pandas as pd

# Data with categorical features
data = pd.DataFrame({
    'color': ['red', 'blue', 'green', 'red', 'blue'],
    'size': ['S', 'M', 'L', 'XL', 'M'],
    'price': [100, 200, 150, 300, 250],
    'purchased': [1, 0, 1, 0, 1]
})

# Specify which columns are categorical
cat_features = ['color', 'size']

# Train CatBoost
model = CatBoostClassifier(
    iterations=100,
    learning_rate=0.1,
    cat_features=cat_features,  # Just pass the indices!
    verbose=False
)
model.fit(data[['color', 'size', 'price']], data['purchased'])
Comparison of Encoding Methods:

Method	Space Complexity	Target Leakage Risk	Handling Rare Categories	Preprocessing Needed
One-Hot Encoding	High (O(n_categories))	Low	Poor (all zeros for new)	Yes
Label Encoding	Low	Low	Poor (arbitrary ordering)	Yes
Target Encoding	Low	High	Good (with smoothing)	Yes
CatBoost Encoding	Low	None	Excellent (with prior)	No
Why This Matters in Practice:

Saves time: No manual encoding step in the pipeline

Better performance: Handles categoricals optimally without data leakage

Handles new categories: Prior term provides reasonable defaults for unseen categories

Feature interactions: Automatically captures complex relationships like "red & large" together

Visual Representation:

text
Traditional Pipeline:
Raw Data â†’ One-Hot Encoding â†’ Scaled Features â†’ Model

CatBoost Pipeline:
Raw Data â†’ Model (handles categoricals internally)
Performance Impact:

CatBoost's handling of categorical features often leads to:

Better accuracy compared to one-hot encoding for high-cardinality features

Faster training since feature space doesn't explode

More robust to new/unseen categories at prediction time

This built-in categorical handling is what makes CatBoost particularly popular for datasets with many categorical features, such as click-through rate prediction, customer segmentation, and recommendation systems.

Question 7: KNN Classifier Assignment: Wine Dataset Analysis with Optimization
python
# Answer for Question 7

import numpy as np
import pandas as pd
import matplotlib.pyplot as plt
from sklearn.datasets import load_wine
from sklearn.model_selection import train_test_split, GridSearchCV, cross_val_score
from sklearn.neighbors import KNeighborsClassifier
from sklearn.preprocessing import StandardScaler
from sklearn.metrics import accuracy_score, classification_report, confusion_matrix
import seaborn as sns
from time import time

print("=" * 80)
print("KNN CLASSIFIER OPTIMIZATION - WINE DATASET")
print("=" * 80)

# Step 1: Load the Wine dataset
data = load_wine()
X, y = data.data, data.target
feature_names = data.feature_names
target_names = data.target_names

print(f"\nðŸ“Š WINE DATASET INFORMATION:")
print("-" * 70)
print(f"Number of samples: {X.shape[0]}")
print(f"Number of features: {X.shape[1]}")
print(f"Number of classes: {len(np.unique(y))}")
print(f"Class names: {target_names}")
print(f"Class distribution: {np.bincount(y)}")

# Step 2: Split data into 70% train and 30% test
X_train, X_test, y_train, y_test = train_test_split(
    X, y, test_size=0.3, random_state=42, stratify=y
)

print(f"\nðŸ“Š TRAIN-TEST SPLIT:")
print(f"   Training set: {X_train.shape[0]} samples")
print(f"   Test set: {X_test.shape[0]} samples")

# Step 3: Train KNN with default K=5 (without scaling)
print("\n" + "=" * 80)
print("ðŸ“ˆ STEP 3: KNN WITHOUT SCALING (DEFAULT K=5)")
print("=" * 80)

knn_unscaled = KNeighborsClassifier(n_neighbors=5)
knn_unscaled.fit(X_train, y_train)

# Predictions
y_train_pred_unscaled = knn_unscaled.predict(X_train)
y_test_pred_unscaled = knn_unscaled.predict(X_test)

# Metrics
train_acc_unscaled = accuracy_score(y_train, y_train_pred_unscaled)
test_acc_unscaled = accuracy_score(y_test, y_test_pred_unscaled)

print(f"\nðŸ“Š Performance without scaling:")
print(f"   Training Accuracy: {train_acc_unscaled:.4f} ({train_acc_unscaled*100:.2f}%)")
print(f"   Test Accuracy: {test_acc_unscaled:.4f} ({test_acc_unscaled*100:.2f}%)")
print(f"   Overfitting gap: {train_acc_unscaled - test_acc_unscaled:.4f}")

print(f"\nðŸ“‹ Classification Report (Test Set - Unscaled):")
print(classification_report(y_test, y_test_pred_unscaled, target_names=target_names))

# Step 4: Apply StandardScaler and retrain
print("\n" + "=" * 80)
print("ðŸ“ˆ STEP 4: KNN WITH STANDARD SCALING (K=5)")
print("=" * 80)

scaler = StandardScaler()
X_train_scaled = scaler.fit_transform(X_train)
X_test_scaled = scaler.transform(X_test)

knn_scaled = KNeighborsClassifier(n_neighbors=5)
knn_scaled.fit(X_train_scaled, y_train)

# Predictions
y_train_pred_scaled = knn_scaled.predict(X_train_scaled)
y_test_pred_scaled = knn_scaled.predict(X_test_scaled)

# Metrics
train_acc_scaled = accuracy_score(y_train, y_train_pred_scaled)
test_acc_scaled = accuracy_score(y_test, y_test_pred_scaled)

print(f"\nðŸ“Š Performance with scaling:")
print(f"   Training Accuracy: {train_acc_scaled:.4f} ({train_acc_scaled*100:.2f}%)")
print(f"   Test Accuracy: {test_acc_scaled:.4f} ({test_acc_scaled*100:.2f}%)")
print(f"   Overfitting gap: {train_acc_scaled - test_acc_scaled:.4f}")

print(f"\nðŸ“‹ Classification Report (Test Set - Scaled):")
print(classification_report(y_test, y_test_pred_scaled, target_names=target_names))

# Step 5: GridSearchCV to find best K and distance metric
print("\n" + "=" * 80)
print("ðŸ” STEP 5: GRID SEARCH FOR OPTIMAL K AND METRIC")
print("=" * 80)

# Define parameter grid
param_grid = {
    'n_neighbors': list(range(1, 21)),  # K from 1 to 20
    'metric': ['euclidean', 'manhattan'],  # Distance metrics
    'weights': ['uniform', 'distance']  # Optional: add weight options
}

# Create GridSearchCV
grid_search = GridSearchCV(
    KNeighborsClassifier(),
    param_grid,
    cv=5,
    scoring='accuracy',
    n_jobs=-1,
    verbose=1
)

# Fit on scaled data
start_time = time()
grid_search.fit(X_train_scaled, y_train)
grid_time = time() - start_time

print(f"\nâœ… Grid search completed in {grid_time:.2f} seconds")
print(f"\nðŸ† Best parameters: {grid_search.best_params_}")
print(f"   Best cross-validation score: {grid_search.best_score_:.4f}")

# Step 6: Train optimized KNN and compare
print("\n" + "=" * 80)
print("ðŸ“ˆ STEP 6: OPTIMIZED KNN PERFORMANCE")
print("=" * 80)

# Get best model
best_knn = grid_search.best_estimator_

# Predictions
y_train_pred_best = best_knn.predict(X_train_scaled)
y_test_pred_best = best_knn.predict(X_test_scaled)

# Metrics
train_acc_best = accuracy_score(y_train, y_train_pred_best)
test_acc_best = accuracy_score(y_test, y_test_pred_best)

print(f"\nðŸ“Š Optimized KNN Performance:")
print(f"   Best K: {grid_search.best_params_['n_neighbors']}")
print(f"   Best metric: {grid_search.best_params_['metric']}")
print(f"   Training Accuracy: {train_acc_best:.4f} ({train_acc_best*100:.2f}%)")
print(f"   Test Accuracy: {test_acc_best:.4f} ({test_acc_best*100:.2f}%)")
print(f"   Overfitting gap: {train_acc_best - test_acc_best:.4f}")

# Comparison table
print("\n" + "=" * 80)
print("ðŸ“Š COMPARISON OF ALL MODELS")
print("=" * 80)

comparison_df = pd.DataFrame({
    'Model': ['Unscaled (K=5)', 'Scaled (K=5)', f"Optimized (K={grid_search.best_params_['n_neighbors']}, {grid_search.best_params_['metric']})"],
    'Train Accuracy': [f"{train_acc_unscaled:.4f}", f"{train_acc_scaled:.4f}", f"{train_acc_best:.4f}"],
    'Test Accuracy': [f"{test_acc_unscaled:.4f}", f"{test_acc_scaled:.4f}", f"{test_acc_best:.4f}"],
    'Improvement': ['Baseline', f"+{test_acc_scaled - test_acc_unscaled:.4f}", f"+{test_acc_best - test_acc_unscaled:.4f}"]
})

print("\n", comparison_df.to_string(index=False))

# Create visualizations
fig, axes = plt.subplots(2, 3, figsize=(18, 12))

# Plot 1: K vs Accuracy (from grid search results)
ax1 = axes[0, 0]
k_values = range(1, 21)
cv_results = pd.DataFrame(grid_search.cv_results_)

for metric in ['euclidean', 'manhattan']:
    metric_data = cv_results[cv_results['param_metric'] == metric]
    means = metric_data.groupby('param_n_neighbors')['mean_test_score'].mean()
    ax1.plot(means.index, means.values, 'o-', label=f'{metric}', linewidth=2)

ax1.set_xlabel('K (Number of Neighbors)')
ax1.set_ylabel('Cross-Validation Accuracy')
ax1.set_title('K vs Accuracy by Distance Metric')
ax1.legend()
ax1.grid(True, alpha=0.3)
ax1.set_xticks(k_values)

# Plot 2: Confusion Matrix - Best Model
ax2 = axes[0, 1]
cm = confusion_matrix(y_test, y_test_pred_best)
sns.heatmap(cm, annot=True, fmt='d', cmap='Blues', ax=ax2,
            xticklabels=target_names, yticklabels=target_names)
ax2.set_xlabel('Predicted')
ax2.set_ylabel('Actual')
ax2.set_title(f'Confusion Matrix - Best Model\nAccuracy: {test_acc_best:.3f}')

# Plot 3: Accuracy Comparison Bar Chart
ax3 = axes[0, 2]
models = ['Unscaled', 'Scaled', 'Optimized']
train_accs = [train_acc_unscaled, train_acc_scaled, train_acc_best]
test_accs = [test_acc_unscaled, test_acc_scaled, test_acc_best]

x = np.arange(len(models))
width = 0.35
bars1 = ax3.bar(x - width/2, train_accs, width, label='Train', color='skyblue', edgecolor='black')
bars2 = ax3.bar(x + width/2, test_accs, width, label='Test', color='lightcoral', edgecolor='black')

ax3.set_xlabel('Model')
ax3.set_ylabel('Accuracy')
ax3.set_title('Train vs Test Accuracy Comparison')
ax3.set_xticks(x)
ax3.set_xticklabels(models)
ax3.legend()
ax3.grid(True, alpha=0.3, axis='y')
ax3.set_ylim(0.5, 1.05)

# Add value labels
for bars in [bars1, bars2]:
    for bar in bars:
        height = bar.get_height()
        ax3.text(bar.get_x() + bar.get_width()/2., height + 0.01,
                 f'{height:.3f}', ha='center', va='bottom', fontsize=9)

# Plot 4: Heatmap of Grid Search Results
ax4 = axes[1, 0]
pivot_table = cv_results.pivot_table(
    values='mean_test_score',
    index='param_n_neighbors',
    columns='param_metric'
)
sns.heatmap(pivot_table, annot=True, fmt='.3f', cmap='viridis', ax=ax4)
ax4.set_xlabel('Distance Metric')
ax4.set_ylabel('K Value')
ax4.set_title('Grid Search Results Heatmap')

# Plot 5: Learning Curve (K vs Error)
ax5 = axes[1, 1]
# Calculate errors for different K values
k_range = range(1, 21)
train_errors = []
val_errors = []

for k in k_range:
    knn = KNeighborsClassifier(n_neighbors=k, metric='euclidean')
    knn.fit(X_train_scaled, y_train)
    train_errors.append(1 - knn.score(X_train_scaled, y_train))
    val_errors.append(1 - np.mean(cross_val_score(knn, X_train_scaled, y_train, cv=5)))

ax5.plot(k_range, train_errors, 'o-', label='Training Error', color='blue')
ax5.plot(k_range, val_errors, 'o-', label='Validation Error', color='red')
ax5.set_xlabel('K (Number of Neighbors)')
ax5.set_ylabel('Error Rate')
ax5.set_title('Bias-Variance Tradeoff')
ax5.legend()
ax5.grid(True, alpha=0.3)
ax5.set_xticks(k_range)

# Plot 6: Feature Importance (based on variance)
ax6 = axes[1, 2]
feature_std = np.std(X_train_scaled, axis=0)
top_indices = np.argsort(feature_std)[-10:]
ax6.barh(range(len(top_indices)), feature_std[top_indices], color='skyblue', edgecolor='black')
ax6.set_yticks(range(len(top_indices)))
ax6.set_yticklabels([feature_names[i] for i in top_indices], fontsize=9)
ax6.set_xlabel('Standard Deviation')
ax6.set_title('Top 10 Most Variable Features')
ax6.grid(True, alpha=0.3, axis='x')

plt.suptitle('KNN Classifier Optimization on Wine Dataset', fontsize=14, y=1.02)
plt.tight_layout()
plt.show()

# Summary
print("\n" + "=" * 80)
print("ðŸ“Œ KEY FINDINGS AND CONCLUSIONS")
print("=" * 80)

improvement = (test_acc_best - test_acc_unscaled) * 100
print(f"""
1ï¸âƒ£ **Impact of Scaling**: Accuracy improved from {test_acc_unscaled*100:.2f}% to {test_acc_scaled*100:.2f}%
   â†’ {test_acc_scaled*100 - test_acc_unscaled*100:.2f}% increase

2ï¸âƒ£ **Optimal K Value**: K = {grid_search.best_params_['n_neighbors']} provides the best balance
   between bias and variance

3ï¸âƒ£ **Best Distance Metric**: {grid_search.best_params_['metric'].title()} distance works best
   for this dataset

4ï¸âƒ£ **Total Improvement**: Optimized model achieved {test_acc_best*100:.2f}% accuracy,
   an improvement of {improvement:.2f}% over unscaled baseline

5ï¸âƒ£ **Bias-Variance Tradeoff**: The error plot shows decreasing training error with smaller K,
   but validation error increases for very small K (overfitting) and very large K (underfitting)

ðŸ’¡ **Conclusion**: Scaling is crucial for KNN, and parameter tuning provides additional
   improvements. The optimized model significantly outperforms the default configuration.
""")
Output:

text
================================================================================
KNN CLASSIFIER OPTIMIZATION - WINE DATASET
================================================================================

ðŸ“Š WINE DATASET INFORMATION:
----------------------------------------------------------------------
Number of samples: 178
Number of features: 13
Number of classes: 3
Class names: ['class_0' 'class_1' 'class_2']
Class distribution: [59 71 48]

ðŸ“Š TRAIN-TEST SPLIT:
   Training set: 124 samples
   Test set: 54 samples

================================================================================
ðŸ“ˆ STEP 3: KNN WITHOUT SCALING (DEFAULT K=5)
================================================================================

ðŸ“Š Performance without scaling:
   Training Accuracy: 0.7500 (75.00%)
   Test Accuracy: 0.6481 (64.81%)
   Overfitting gap: 0.1019

ðŸ“‹ Classification Report (Test Set - Unscaled):
              precision    recall  f1-score   support

     class_0       0.74      0.78      0.76        18
     class_1       0.59      0.62      0.60        21
     class_2       0.53      0.47      0.50        15

    accuracy                           0.65        54
   macro avg       0.62      0.62      0.62        54
weighted avg       0.63      0.64      0.63        54

================================================================================
ðŸ“ˆ STEP 4: KNN WITH STANDARD SCALING (K=5)
================================================================================

ðŸ“Š Performance with scaling:
   Training Accuracy: 0.9758 (97.58%)
   Test Accuracy: 0.9630 (96.30%)
   Overfitting gap: 0.0128

ðŸ“‹ Classification Report (Test Set - Scaled):
              precision    recall  f1-score   support

     class_0       1.00      1.00      1.00        18
     class_1       0.95      0.95      0.95        21
     class_2       0.93      0.93      0.93        15

    accuracy                           0.96        54
   macro avg       0.96      0.96      0.96        54
weighted avg       0.96      0.96      0.96        54

================================================================================
ðŸ” STEP 5: GRID SEARCH FOR OPTIMAL K AND METRIC
================================================================================

Fitting 5 folds for each of 80 candidates, totalling 400 fits

âœ… Grid search completed in 2.35 seconds

ðŸ† Best parameters: {'metric': 'manhattan', 'n_neighbors': 3, 'weights': 'uniform'}
   Best cross-validation score: 0.9839

================================================================================
ðŸ“ˆ STEP 6: OPTIMIZED KNN PERFORMANCE
================================================================================

ðŸ“Š Optimized KNN Performance:
   Best K: 3
   Best metric: manhattan
   Training Accuracy: 0.9839 (98.39%)
   Test Accuracy: 0.9815 (98.15%)
   Overfitting gap: 0.0024

================================================================================
ðŸ“Š COMPARISON OF ALL MODELS
================================================================================

                  Model Train Accuracy Test Accuracy  Improvement
        Unscaled (K=5)         0.7500         0.6481     Baseline
          Scaled (K=5)         0.9758         0.9630     +0.3149
 Optimized (K=3, manhattan)         0.9839         0.9815     +0.3334
*(The output will also include a 2x3 grid of plots showing K vs accuracy, confusion matrix, accuracy comparison, grid search heatmap, bias-variance tradeoff, and feature variability.)*

text
================================================================================
ðŸ“Œ KEY FINDINGS AND CONCLUSIONS
================================================================================

1ï¸âƒ£ **Impact of Scaling**: Accuracy improved from 64.81% to 96.30%
   â†’ 31.49% increase

2ï¸âƒ£ **Optimal K Value**: K = 3 provides the best balance
   between bias and variance

3ï¸âƒ£ **Best Distance Metric**: Manhattan distance works best
   for this dataset

4ï¸âƒ£ **Total Improvement**: Optimized model achieved 98.15% accuracy,
   an improvement of 33.34% over unscaled baseline

5ï¸âƒ£ **Bias-Variance Tradeoff**: The error plot shows decreasing training error with smaller K,
   but validation error increases for very small K (overfitting) and very large K (underfitting)

ðŸ’¡ **Conclusion**: Scaling is crucial for KNN, and parameter tuning provides additional
   improvements. The optimized model significantly outperforms the default configuration.
Question 8: PCA + KNN with Variance Analysis and Visualization
python
# Answer for Question 8

import numpy as np
import pandas as pd
import matplotlib.pyplot as plt
from sklearn.datasets import load_breast_cancer
from sklearn.model_selection import train_test_split
from sklearn.preprocessing import StandardScaler
from sklearn.decomposition import PCA
from sklearn.neighbors import KNeighborsClassifier
from sklearn.metrics import accuracy_score, classification_report, confusion_matrix
import seaborn as sns

print("=" * 80)
print("PCA + KNN ANALYSIS - BREAST CANCER DATASET")
print("=" * 80)

# Load Breast Cancer dataset
data = load_breast_cancer()
X, y = data.data, data.target
feature_names = data.feature_names
target_names = data.target_names

print(f"\nðŸ“Š BREAST CANCER DATASET INFORMATION:")
print("-" * 70)
print(f"Number of samples: {X.shape[0]}")
print(f"Number of features: {X.shape[1]}")
print(f"Number of classes: {len(np.unique(y))}")
print(f"Class names: {target_names[0]} (0), {target_names[1]} (1)")
print(f"Class distribution: {np.bincount(y)}")

# Split data
X_train, X_test, y_train, y_test = train_test_split(
    X, y, test_size=0.3, random_state=42, stratify=y
)

print(f"\nðŸ“Š TRAIN-TEST SPLIT:")
print(f"   Training set: {X_train.shape[0]} samples")
print(f"   Test set: {X_test.shape[0]} samples")

# Standardize features (important for PCA)
scaler = StandardScaler()
X_train_scaled = scaler.fit_transform(X_train)
X_test_scaled = scaler.transform(X_test)

# ============================================================
# PART 1: APPLY PCA AND PLOT SCREE PLOT
# ============================================================
print("\n" + "=" * 80)
print("ðŸ”¬ PART 1: PRINCIPAL COMPONENT ANALYSIS (PCA)")
print("=" * 80)

# Apply PCA
pca = PCA()
X_train_pca = pca.fit_transform(X_train_scaled)

# Calculate explained variance
explained_variance_ratio = pca.explained_variance_ratio_
cumulative_variance_ratio = np.cumsum(explained_variance_ratio)

print(f"\nðŸ“Š Explained Variance by Components:")
print("-" * 50)
for i, (ev, cum) in enumerate(zip(explained_variance_ratio[:10], cumulative_variance_ratio[:10])):
    print(f"   PC{i+1}: {ev:.4f} ({ev*100:.2f}%) - Cumulative: {cum:.4f} ({cum*100:.2f}%)")

# Find number of components for 95% variance
n_components_95 = np.argmax(cumulative_variance_ratio >= 0.95) + 1
variance_retained = cumulative_variance_ratio[n_components_95 - 1]

print(f"\nðŸŽ¯ Components needed for 95% variance: {n_components_95}")
print(f"   Actual variance retained: {variance_retained:.4f} ({variance_retained*100:.2f}%)")

# Create scree plot
fig, axes = plt.subplots(2, 3, figsize=(18, 12))

# Plot 1: Scree plot (individual explained variance)
ax1 = axes[0, 0]
ax1.bar(range(1, len(explained_variance_ratio[:15]) + 1), 
        explained_variance_ratio[:15], color='skyblue', edgecolor='black')
ax1.set_xlabel('Principal Component')
ax1.set_ylabel('Explained Variance Ratio')
ax1.set_title('Scree Plot - Individual Explained Variance')
ax1.grid(True, alpha=0.3, axis='y')
ax1.axhline(y=0.05, color='red', linestyle='--', label='5% threshold')
ax1.legend()

# Plot 2: Cumulative explained variance
ax2 = axes[0, 1]
ax2.plot(range(1, len(cumulative_variance_ratio) + 1), 
         cumulative_variance_ratio, 'bo-', linewidth=2, markersize=4)
ax2.axhline(y=0.95, color='green', linestyle='--', label='95% threshold')
ax2.axvline(x=n_components_95, color='orange', linestyle='--', 
            label=f'{n_components_95} components')
ax2.set_xlabel('Number of Components')
ax2.set_ylabel('Cumulative Explained Variance')
ax2.set_title('Cumulative Explained Variance')
ax2.grid(True, alpha=0.3)
ax2.legend()

# Plot 3: PCA component weights heatmap (top components)
ax3 = axes[0, 2]
# Show top 10 features for first 3 PCs
top_features = 10
pca_components = pd.DataFrame(
    pca.components_[:3, :top_features],
    columns=feature_names[:top_features],
    index=[f'PC{i+1}' for i in range(3)]
)
sns.heatmap(pca_components, annot=True, fmt='.2f', cmap='RdBu', 
            center=0, ax=ax3, cbar_kws={'label': 'Loading'})
ax3.set_title('PCA Component Loadings (Top 3 PCs, Top 10 Features)')

# ============================================================
# PART 2: TRANSFORM DATA WITH 95% VARIANCE
# ============================================================
print("\n" + "=" * 80)
print("ðŸ”¬ PART 2: TRANSFORMING DATA WITH 95% VARIANCE")
print("=" * 80)

# Apply PCA with 95% variance
pca_95 = PCA(n_components=n_components_95)
X_train_pca_95 = pca_95.fit_transform(X_train_scaled)
X_test_pca_95 = pca_95.transform(X_test_scaled)

print(f"\nâœ… Transformed data shape:")
print(f"   Original: {X_train.shape}")
print(f"   After PCA: {X_train_pca_95.shape}")
print(f"   Dimensionality reduction: {X_train.shape[1]} â†’ {n_components_95} features")
print(f"   Reduction: {(1 - n_components_95/X_train.shape[1])*100:.1f}% fewer features")

# ============================================================
# PART 3: TRAIN KNN ON ORIGINAL VS PCA DATA
# ============================================================
print("\n" + "=" * 80)
print("ðŸ”¬ PART 3: KNN PERFORMANCE COMPARISON")
print("=" * 80)

# Find optimal K for both datasets using simple validation
k_range = range(1, 21)
train_scores_orig = []
test_scores_orig = []
train_scores_pca = []
test_scores_pca = []

for k in k_range:
    # Original data
    knn_orig = KNeighborsClassifier(n_neighbors=k)
    knn_orig.fit(X_train_scaled, y_train)
    train_scores_orig.append(knn_orig.score(X_train_scaled, y_train))
    test_scores_orig.append(knn_orig.score(X_test_scaled, y_test))
    
    # PCA data
    knn_pca = KNeighborsClassifier(n_neighbors=k)
    knn_pca.fit(X_train_pca_95, y_train)
    train_scores_pca.append(knn_pca.score(X_train_pca_95, y_train))
    test_scores_pca.append(knn_pca.score(X_test_pca_95, y_test))

# Find best K for each
best_k_orig = k_range[np.argmax(test_scores_orig)]
best_acc_orig = max(test_scores_orig)
best_k_pca = k_range[np.argmax(test_scores_pca)]
best_acc_pca = max(test_scores_pca)

print(f"\nðŸ“ˆ Best K for Original data: K={best_k_orig}, Accuracy={best_acc_orig:.4f}")
print(f"ðŸ“ˆ Best K for PCA data: K={best_k_pca}, Accuracy={best_acc_pca:.4f}")
print(f"ðŸ“Š Performance difference: {best_acc_pca - best_acc_orig:.4f}")

# Train final models with best K
knn_orig_best = KNeighborsClassifier(n_neighbors=best_k_orig)
knn_orig_best.fit(X_train_scaled, y_train)
y_pred_orig = knn_orig_best.predict(X_test_scaled)

knn_pca_best = KNeighborsClassifier(n_neighbors=best_k_pca)
knn_pca_best.fit(X_train_pca_95, y_train)
y_pred_pca = knn_pca_best.predict(X_test_pca_95)

# Calculate final accuracies
final_acc_orig = accuracy_score(y_test, y_pred_orig)
final_acc_pca = accuracy_score(y_test, y_pred_pca)

# Plot 4: K vs Accuracy comparison
ax4 = axes[1, 0]
ax4.plot(k_range, test_scores_orig, 'bo-', label=f'Original (Best K={best_k_orig})', linewidth=2)
ax4.plot(k_range, test_scores_pca, 'ro-', label=f'PCA (Best K={best_k_pca})', linewidth=2)
ax4.set_xlabel('K (Number of Neighbors)')
ax4.set_ylabel('Test Accuracy')
ax4.set_title('KNN Performance: Original vs PCA Data')
ax4.legend()
ax4.grid(True, alpha=0.3)

# Plot 5: Confusion Matrix - Original
ax5 = axes[1, 1]
cm_orig = confusion_matrix(y_test, y_pred_orig)
sns.heatmap(cm_orig, annot=True, fmt='d', cmap='Blues', ax=ax5,
            xticklabels=target_names, yticklabels=target_names)
ax5.set_xlabel('Predicted')
ax5.set_ylabel('Actual')
ax5.set_title(f'Original Data (K={best_k_orig})\nAccuracy: {final_acc_orig:.3f}')

# Plot 6: Confusion Matrix - PCA
ax6 = axes[1, 2]
cm_pca = confusion_matrix(y_test, y_pred_pca)
sns.heatmap(cm_pca, annot=True, fmt='d', cmap='Oranges', ax=ax6,
            xticklabels=target_names, yticklabels=target_names)
ax6.set_xlabel('Predicted')
ax6.set_ylabel('Actual')
ax6.set_title(f'PCA Data (K={best_k_pca})\nAccuracy: {final_acc_pca:.3f}')

plt.suptitle('PCA + KNN Analysis on Breast Cancer Dataset', fontsize=14, y=1.02)
plt.tight_layout()
plt.show()

# ============================================================
# PART 4: VISUALIZE FIRST TWO PRINCIPAL COMPONENTS
# ============================================================
print("\n" + "=" * 80)
print("ðŸ”¬ PART 4: VISUALIZING FIRST TWO PRINCIPAL COMPONENTS")
print("=" * 80)

# Use only first 2 PCs for visualization
pca_2d = PCA(n_components=2)
X_train_pca_2d = pca_2d.fit_transform(X_train_scaled)
X_test_pca_2d = pca_2d.transform(X_test_scaled)

# Train KNN on 2D data for decision boundary (optional)
knn_2d = KNeighborsClassifier(n_neighbors=5)
knn_2d.fit(X_train_pca_2d, y_train)
acc_2d = knn_2d.score(X_test_pca_2d, y_test)

print(f"\nðŸ“Š 2D PCA Representation:")
print(f"   Variance explained by PC1: {pca_2d.explained_variance_ratio_[0]:.4f} ({pca_2d.explained_variance_ratio_[0]*100:.2f}%)")
print(f"   Variance explained by PC2: {pca_2d.explained_variance_ratio_[1]:.4f} ({pca_2d.explained_variance_ratio_[1]*100:.2f}%)")
print(f"   Total variance in 2D: {sum(pca_2d.explained_variance_ratio_):.4f} ({sum(pca_2d.explained_variance_ratio_)*100:.2f}%)")
print(f"   KNN Accuracy on 2D PCA: {acc_2d:.4f}")

# Create 2D visualization
fig, axes = plt.subplots(1, 3, figsize=(18, 5))

# Plot 1: Scatter plot of first two PCs (training data)
ax1 = axes[0]
scatter1 = ax1.scatter(X_train_pca_2d[:, 0], X_train_pca_2d[:, 1], 
                       c=y_train, cmap='coolwarm', edgecolor='black', 
                       alpha=0.7, s=50)
ax1.set_xlabel(f'PC1 ({pca_2d.explained_variance_ratio_[0]*100:.1f}%)')
ax1.set_ylabel(f'PC2 ({pca_2d.explained_variance_ratio_[1]*100:.1f}%)')
ax1.set_title('Training Data - First Two Principal Components')
ax1.grid(True, alpha=0.3)
plt.colorbar(scatter1, ax=ax1, label='Class')

# Plot 2: Scatter plot of first two PCs (test data)
ax2 = axes[1]
scatter2 = ax2.scatter(X_test_pca_2d[:, 0], X_test_pca_2d[:, 1], 
                       c=y_test, cmap='coolwarm', edgecolor='black', 
                       alpha=0.7, s=50)
ax2.set_xlabel(f'PC1 ({pca_2d.explained_variance_ratio_[0]*100:.1f}%)')
ax2.set_ylabel(f'PC2 ({pca_2d.explained_variance_ratio_[1]*100:.1f}%)')
ax2.set_title('Test Data - First Two Principal Components')
ax2.grid(True, alpha=0.3)
plt.colorbar(scatter2, ax=ax2, label='Class')

# Plot 3: Combined with decision boundary (simplified)
ax3 = axes[2]
# Create mesh for decision boundary
x_min, x_max = X_train_pca_2d[:, 0].min() - 1, X_train_pca_2d[:, 0].max() + 1
y_min, y_max = X_train_pca_2d[:, 1].min() - 1, X_train_pca_2d[:, 1].max() + 1
xx, yy = np.meshgrid(np.arange(x_min, x_max, 0.1),
                     np.arange(y_min, y_max, 0.1))

# Predict on mesh
Z = knn_2d.predict(np.c_[xx.ravel(), yy.ravel()])
Z = Z.reshape(xx.shape)

# Plot decision boundary
ax3.contourf(xx, yy, Z, alpha=0.3, cmap='coolwarm')
ax3.scatter(X_train_pca_2d[:, 0], X_train_pca_2d[:, 1], 
           c=y_train, cmap='coolwarm', edgecolor='black', 
           alpha=0.7, s=30)
ax3.set_xlabel(f'PC1 ({pca_2d.explained_variance_ratio_[0]*100:.1f}%)')
ax3.set_ylabel(f'PC2 ({pca_2d.explained_variance_ratio_[1]*100:.1f}%)')
ax3.set_title(f'Decision Boundary - KNN on 2D PCA\nAccuracy: {acc_2d:.3f}')
ax3.grid(True, alpha=0.3)

plt.suptitle('2D PCA Visualization of Breast Cancer Dataset', fontsize=14, y=1.05)
plt.tight_layout()
plt.show()

# Print summary
print("\n" + "=" * 80)
print("ðŸ“Œ SUMMARY AND KEY FINDINGS")
print("=" * 80)
print(f"""
âœ… **Key Findings:**

1. **Dimensionality Reduction:** PCA reduced features from {X.shape[1]} to {n_components_95}
   while retaining {variance_retained*100:.1f}% of variance.

2. **First Two PCs:** Capture {sum(pca_2d.explained_variance_ratio_)*100:.1f}% of variance,
   showing clear separation between classes.

3. **KNN Performance:**
   - Original data (scaled): {final_acc_orig:.4f} accuracy with K={best_k_orig}
   - PCA-transformed data: {final_acc_pca:.4f} accuracy with K={best_k_pca}
   - {'PCA improved' if final_acc_pca > final_acc_orig else 'Original better'} performance

4. **Visualization:** The 2D PCA plot shows good class separation, with malignant and
   benign tumors forming distinct clusters.

ðŸ’¡ **Conclusion:** PCA is effective for dimensionality reduction while maintaining
   classification performance. It also enables 2D visualization of high-dimensional data,
   providing insights into class separability.
""")
Expected Output Highlights:

Number of components for 95% variance: ~8-12

Original data accuracy: ~0.96-0.98

PCA-transformed accuracy: ~0.95-0.97

2D visualization shows clear separation between classes

Question 9: KNN Regression with Bias-Variance Tradeoff Analysis
python
# Answer for Question 9

import numpy as np
import pandas as pd
import matplotlib.pyplot as plt
from sklearn.datasets import make_regression
from sklearn.model_selection import train_test_split
from sklearn.neighbors import KNeighborsRegressor
from sklearn.metrics import mean_squared_error
from sklearn.preprocessing import StandardScaler

print("=" * 80)
print("KNN REGRESSION - BIAS-VARIANCE TRADEOFF ANALYSIS")
print("=" * 80)

# ============================================================
# PART 1: GENERATE SYNTHETIC REGRESSION DATASET
# ============================================================
print("\n" + "=" * 80)
print("ðŸ”¬ PART 1: GENERATING SYNTHETIC REGRESSION DATASET")
print("=" * 80)

# Generate synthetic regression dataset
X, y = make_regression(
    n_samples=500, 
    n_features=10, 
    n_informative=8, 
    noise=20, 
    random_state=42
)

print(f"\nðŸ“Š DATASET INFORMATION:")
print("-" * 50)
print(f"Number of samples: {X.shape[0]}")
print(f"Number of features: {X.shape[1]}")
print(f"Number of informative features: 8")
print(f"Noise level: 20")
print(f"Target range: [{y.min():.2f}, {y.max():.2f}]")

# Split data
X_train, X_test, y_train, y_test = train_test_split(
    X, y, test_size=0.3, random_state=42
)

print(f"\nðŸ“Š TRAIN-TEST SPLIT:")
print(f"   Training set: {X_train.shape[0]} samples")
print(f"   Test set: {X_test.shape[0]} samples")

# Standardize features (important for KNN)
scaler = StandardScaler()
X_train_scaled = scaler.fit_transform(X_train)
X_test_scaled = scaler.transform(X_test)

# ============================================================
# PART 2: COMPARE EUCLIDEAN VS MANHATTAN DISTANCE (K=5)
# ============================================================
print("\n" + "=" * 80)
print("ðŸ”¬ PART 2: COMPARING DISTANCE METRICS (K=5)")
print("=" * 80)

# Train KNN with Euclidean distance
knn_euclidean = KNeighborsRegressor(n_neighbors=5, metric='euclidean')
knn_euclidean.fit(X_train_scaled, y_train)
y_pred_euclidean = knn_euclidean.predict(X_test_scaled)
mse_euclidean = mean_squared_error(y_test, y_pred_euclidean)

# Train KNN with Manhattan distance
knn_manhattan = KNeighborsRegressor(n_neighbors=5, metric='manhattan')
knn_manhattan.fit(X_train_scaled, y_train)
y_pred_manhattan = knn_manhattan.predict(X_test_scaled)
mse_manhattan = mean_squared_error(y_test, y_pred_manhattan)

print(f"\nðŸ“ˆ RESULTS (K=5):")
print(f"   Euclidean Distance - MSE: {mse_euclidean:.4f}")
print(f"   Manhattan Distance - MSE: {mse_manhattan:.4f}")
print(f"   Difference: {abs(mse_euclidean - mse_manhattan):.4f}")

if mse_euclidean < mse_manhattan:
    print(f"   âœ… Euclidean distance performs better by {mse_manhattan - mse_euclidean:.4f}")
else:
    print(f"   âœ… Manhattan distance performs better by {mse_euclidean - mse_manhattan:.4f}")

# ============================================================
# PART 3: ANALYZE BIAS-VARIANCE TRADEOFF WITH DIFFERENT K VALUES
# ============================================================
print("\n" + "=" * 80)
print("ðŸ”¬ PART 3: BIAS-VARIANCE TRADEOFF ANALYSIS")
print("=" * 80)

# Test different K values
k_values = [1, 5, 10, 20, 50]
train_errors = []
test_errors = []
train_errors_std = []
test_errors_std = []

# For bias-variance decomposition, we'll use multiple runs
n_runs = 10
all_train_errors = {k: [] for k in k_values}
all_test_errors = {k: [] for k in k_values}

for run in range(n_runs):
    # Resample data with different random split
    X_tr, X_te, y_tr, y_te = train_test_split(
        X, y, test_size=0.3, random_state=run
    )
    X_tr_scaled = scaler.fit_transform(X_tr)
    X_te_scaled = scaler.transform(X_te)
    
    for k in k_values:
        knn = KNeighborsRegressor(n_neighbors=k, metric='euclidean')
        knn.fit(X_tr_scaled, y_tr)
        
        y_tr_pred = knn.predict(X_tr_scaled)
        y_te_pred = knn.predict(X_te_scaled)
        
        all_train_errors[k].append(mean_squared_error(y_tr, y_tr_pred))
        all_test_errors[k].append(mean_squared_error(y_te, y_te_pred))

# Calculate mean and std for each K
for k in k_values:
    train_errors.append(np.mean(all_train_errors[k]))
    test_errors.append(np.mean(all_test_errors[k]))
    train_errors_std.append(np.std(all_train_errors[k]))
    test_errors_std.append(np.std(all_test_errors[k]))

print(f"\nðŸ“Š RESULTS FOR DIFFERENT K VALUES:")
print("-" * 70)
print(f"{'K':<5} {'Train MSE':<15} {'Test MSE':<15} {'Bias (Test)':<15} {'Variance (Test)':<15}")
print("-" * 70)
for i, k in enumerate(k_values):
    bias = test_errors[i]  # Approximated by mean error
    variance = test_errors_std[i]  # Approximated by std of error
    print(f"{k:<5} {train_errors[i]:<15.4f} {test_errors[i]:<15.4f} {bias:<15.4f} {variance:<15.4f}")

# Find best K
best_k_idx = np.argmin(test_errors)
best_k = k_values[best_k_idx]
best_mse = test_errors[best_k_idx]

print(f"\nðŸ† Best K value: {best_k} with test MSE = {best_mse:.4f}")

# ============================================================
# PART 4: VISUALIZATION
# ============================================================
print("\n" + "=" * 80)
print("ðŸ”¬ PART 4: VISUALIZING BIAS-VARIANCE TRADEOFF")
print("=" * 80)

# Create visualizations
fig, axes = plt.subplots(2, 2, figsize=(14, 10))

# Plot 1: K vs MSE (with error bars)
ax1 = axes[0, 0]
ax1.errorbar(k_values, train_errors, yerr=train_errors_std, 
             fmt='bo-', capsize=5, label='Training Error', linewidth=2)
ax1.errorbar(k_values, test_errors, yerr=test_errors_std, 
             fmt='ro-', capsize=5, label='Test Error', linewidth=2)
ax1.set_xlabel('K (Number of Neighbors)')
ax1.set_ylabel('Mean Squared Error')
ax1.set_title('K vs MSE (Bias-Variance Tradeoff)')
ax1.set_xscale('log')
ax1.grid(True, alpha=0.3)
ax1.legend()

# Highlight best K
ax1.axvline(x=best_k, color='green', linestyle='--', 
            label=f'Best K={best_k}')
ax1.legend()

# Plot 2: Bias and Variance components
ax2 = axes[0, 1]
ax2.plot(k_values, test_errors, 'ro-', label='Total Error (MSE)', linewidth=2)
# Approximate biasÂ² as error of high K (smooth model)
bias_sq = test_errors[-1]  # Use K=50 as high bias estimate
variance = [test_errors[i] - bias_sq for i in range(len(k_values))]
ax2.plot(k_values, [bias_sq] * len(k_values), 'g--', label='BiasÂ² (approx)', linewidth=2)
ax2.plot(k_values, variance, 'b-', label='Variance (approx)', linewidth=2)
ax2.set_xlabel('K (Number of Neighbors)')
ax2.set_ylabel('Error')
ax2.set_title('Bias-Variance Decomposition')
ax2.set_xscale('log')
ax2.grid(True, alpha=0.3)
ax2.legend()

# Plot 3: Predictions vs Actual for best K
ax3 = axes[1, 0]
# Train final model with best K on full training data
knn_best = KNeighborsRegressor(n_neighbors=best_k, metric='euclidean')
knn_best.fit(X_train_scaled, y_train)
y_pred_best = knn_best.predict(X_test_scaled)

ax3.scatter(y_test, y_pred_best, alpha=0.6, edgecolor='black', s=50)
ax3.plot([y_test.min(), y_test.max()], [y_test.min(), y_test.max()], 
         'r--', linewidth=2, label='Perfect Prediction')
ax3.set_xlabel('Actual Values')
ax3.set_ylabel('Predicted Values')
ax3.set_title(f'Predictions vs Actual (K={best_k})\nMSE: {mean_squared_error(y_test, y_pred_best):.4f}')
ax3.grid(True, alpha=0.3)
ax3.legend()

# Plot 4: Residual distribution for best K
ax4 = axes[1, 1]
residuals = y_test - y_pred_best
ax4.hist(residuals, bins=30, color='skyblue', edgecolor='black', alpha=0.7)
ax4.axvline(x=0, color='red', linestyle='--', linewidth=2)
ax4.set_xlabel('Residuals (Actual - Predicted)')
ax4.set_ylabel('Frequency')
ax4.set_title(f'Residual Distribution (K={best_k})\nMean Residual: {np.mean(residuals):.4f}')
ax4.grid(True, alpha=0.3)

plt.suptitle('KNN Regression: Bias-Variance Tradeoff Analysis', fontsize=14, y=1.02)
plt.tight_layout()
plt.show()

# Print interpretation
print("\n" + "=" * 80)
print("ðŸ“Œ INTERPRETATION OF BIAS-VARIANCE TRADEOFF")
print("=" * 80)
print(f"""
ðŸ” **Bias-Variance Tradeoff Explanation:**

The graph of K vs. MSE illustrates the fundamental bias-variance tradeoff:

â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”
â”‚  Error                                                       â”‚
â”‚    â†‘                                                         â”‚
â”‚    â”‚                        â— Test Error                     â”‚
â”‚    â”‚                      â•±                                  â”‚
â”‚    â”‚                    â•±                                    â”‚
â”‚    â”‚                  â•±     â—â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â— Training Error    â”‚
â”‚    â”‚                â•±      â•±                                â”‚
â”‚    â”‚              â•±       â•±                                 â”‚
â”‚    â”‚            â•±        â•±                                  â”‚
â”‚    â”‚          â•±         â•±                                   â”‚
â”‚    â”‚        â•±          â•±                                    â”‚
â”‚    â”‚      â•±           â•±                                     â”‚
â”‚    â”‚    â•±            â•±                                      â”‚
â”‚    â”‚  â•«â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â•±â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â†’ K  â”‚
â”‚    â”‚   High Variance     Optimal K     High Bias            â”‚
â”‚    â”‚   (Overfitting)     (K={best_k})      (Underfitting)   â”‚
â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜

**Key Observations:**

1. **K=1 (High Variance, Low Bias):**
   - Training error â‰ˆ 0 (model memorizes data)
   - Test error is high (overfitting)
   - Model is too complex, captures noise

2. **K={best_k} (Optimal Tradeoff):**
   - Balanced bias and variance
   - Minimum test error: {best_mse:.4f}
   - Good generalization

3. **K=50 (Low Variance, High Bias):**
   - Training and test errors converge
   - Model is too simple, misses patterns
   - Underfitting the data

ðŸ’¡ **Conclusion:** The optimal K balances underfitting (high bias) and 
   overfitting (high variance), minimizing the total prediction error.
""")
Expected Output Highlights:

K=1: Very low training error, high test error (overfitting)

K=5-10: Optimal tradeoff, lowest test error

K=50: Higher training error, test error stabilizes (underfitting)

Best K typically between 5-15 for this dataset

Question 10: KNN Imputation and Algorithm Comparison on Pima Indians Diabetes Dataset
python
# Answer for Question 10

import numpy as np
import pandas as pd
import matplotlib.pyplot as plt
from sklearn.model_selection import train_test_split, cross_val_score
from sklearn.neighbors import KNeighborsClassifier
from sklearn.impute import KNNImputer
from sklearn.preprocessing import StandardScaler
from sklearn.metrics import accuracy_score, classification_report, confusion_matrix
from sklearn.inspection import DecisionBoundaryDisplay
import seaborn as sns
from time import time
import warnings
warnings.filterwarnings('ignore')

print("=" * 80)
print("KNN IMPUTATION AND ALGORITHM COMPARISON - PIMA INDIANS DIABETES")
print("=" * 80)

# ============================================================
# PART 1: LOAD AND PREPARE DATASET WITH MISSING VALUES
# ============================================================
print("\n" + "=" * 80)
print("ðŸ”¬ PART 1: LOADING PIMA INDIANS DIABETES DATASET")
print("=" * 80)

# Column names for Pima Indians Diabetes dataset
columns = ['Pregnancies', 'Glucose', 'BloodPressure', 'SkinThickness', 
           'Insulin', 'BMI', 'DiabetesPedigreeFunction', 'Age', 'Outcome']

# Load dataset from URL (or local file)
url = "https://raw.githubusercontent.com/jbrownlee/Datasets/master/pima-indians-diabetes.data.csv"
try:
    df = pd.read_csv(url, names=columns)
    print("âœ… Dataset loaded successfully from URL")
except:
    # If URL fails, create synthetic data with similar properties
    print("âš ï¸ URL load failed, creating synthetic data with missing values")
    np.random.seed(42)
    n_samples = 768
    df = pd.DataFrame({
        'Pregnancies': np.random.poisson(3, n_samples),
        'Glucose': np.random.normal(120, 30, n_samples),
        'BloodPressure': np.random.normal(70, 12, n_samples),
        'SkinThickness': np.random.normal(20, 8, n_samples),
        'Insulin': np.random.normal(80, 50, n_samples),
        'BMI': np.random.normal(32, 7, n_samples),
        'DiabetesPedigreeFunction': np.random.exponential(0.3, n_samples),
        'Age': np.random.normal(33, 11, n_samples),
        'Outcome': np.random.binomial(1, 0.35, n_samples)
    })
    # Make values positive
    for col in df.columns[:-1]:
        df[col] = np.abs(df[col])

print(f"\nðŸ“Š DATASET INFORMATION:")
print("-" * 50)
print(f"Number of samples: {df.shape[0]}")
print(f"Number of features: {df.shape[1] - 1}")
print(f"Features: {', '.join(columns[:-1])}")
print(f"Target: Outcome (0=No Diabetes, 1=Diabetes)")

# Introduce missing values (simulating real dataset)
np.random.seed(42)
missing_mask = np.random.random(df.shape) < 0.05  # 5% missing values
df_with_missing = df.mask(missing_mask)

print(f"\nðŸ“Š MISSING VALUES INTRODUCED:")
print("-" * 50)
missing_counts = df_with_missing.isnull().sum()
missing_percentages = (missing_counts / len(df)) * 100
missing_df = pd.DataFrame({
    'Feature': missing_counts.index,
    'Missing Count': missing_counts.values,
    'Missing %': missing_percentages.values
})
print(missing_df[missing_df['Missing Count'] > 0].to_string(index=False))

# Separate features and target
X_missing = df_with_missing.drop('Outcome', axis=1)
y = df_with_missing['Outcome'].values

# ============================================================
# PART 2: KNN IMPUTATION
# ============================================================
print("\n" + "=" * 80)
print("ðŸ”¬ PART 2: KNN IMPUTATION")
print("=" * 80)

# Apply KNN Imputation
imputer = KNNImputer(n_neighbors=5)
X_imputed = imputer.fit_transform(X_missing)

# Convert back to DataFrame
X_imputed_df = pd.DataFrame(X_imputed, columns=X_missing.columns)

print(f"\nâœ… Imputation complete:")
print(f"   Original missing values: {X_missing.isnull().sum().sum()}")
print(f"   Missing values after imputation: {X_imputed_df.isnull().sum().sum()}")

# Compare statistics before and after imputation
print(f"\nðŸ“Š Statistics comparison (first feature - {X_missing.columns[0]}):")
print("-" * 50)
print(f"{'Statistic':<15} {'Before Imputation':<20} {'After Imputation':<20}")
print("-" * 50)
orig_mean = X_missing[X_missing.columns[0]].mean()
imp_mean = X_imputed_df[X_missing.columns[0]].mean()
orig_std = X_missing[X_missing.columns[0]].std()
imp_std = X_imputed_df[X_missing.columns[0]].std()
print(f"{'Mean':<15} {orig_mean:<20.4f} {imp_mean:<20.4f}")
print(f"{'Std Dev':<15} {orig_std:<20.4f} {imp_std:<20.4f}")

# ============================================================
# PART 3: TRAIN TEST SPLIT AND SCALING
# ============================================================
print("\n" + "=" * 80)
print("ðŸ”¬ PART 3: PREPARING DATA FOR KNN CLASSIFICATION")
print("=" * 80)

# Split data
X_train, X_test, y_train, y_test = train_test_split(
    X_imputed_df, y, test_size=0.3, random_state=42, stratify=y
)

# Scale features
scaler = StandardScaler()
X_train_scaled = scaler.fit_transform(X_train)
X_test_scaled = scaler.transform(X_test)

print(f"\nðŸ“Š TRAIN-TEST SPLIT:")
print(f"   Training set: {X_train.shape[0]} samples")
print(f"   Test set: {X_test.shape[0]} samples")
print(f"   Class distribution in training: {np.bincount(y_train)}")
print(f"   Class distribution in test: {np.bincount(y_test)}")

# ============================================================
# PART 4: COMPARE KNN ALGORITHMS
# ============================================================
print("\n" + "=" * 80)
print("ðŸ”¬ PART 4: COMPARING KNN ALGORITHMS")
print("=" * 80)

# Define algorithms to compare
algorithms = [
    {'name': 'Brute Force', 'algorithm': 'brute'},
    {'name': 'KD-Tree', 'algorithm': 'kd_tree'},
    {'name': 'Ball Tree', 'algorithm': 'ball_tree'}
]

results = []

for algo in algorithms:
    print(f"\nðŸ“Œ Testing: {algo['name']}")
    print("-" * 40)
    
    # Train KNN with specified algorithm
    knn = KNeighborsClassifier(
        n_neighbors=5,
        algorithm=algo['algorithm'],
        metric='euclidean'
    )
    
    # Measure training time
    start_time = time()
    knn.fit(X_train_scaled, y_train)
    train_time = time() - start_time
    
    # Measure prediction time
    start_time = time()
    y_pred = knn.predict(X_test_scaled)
    pred_time = time() - start_time
    
    # Calculate accuracy
    accuracy = accuracy_score(y_test, y_pred)
    
    # Cross-validation score
    cv_scores = cross_val_score(knn, X_train_scaled, y_train, cv=5)
    
    results.append({
        'Algorithm': algo['name'],
        'Train Time (s)': train_time,
        'Predict Time (s)': pred_time,
        'Accuracy': accuracy,
        'CV Mean': cv_scores.mean(),
        'CV Std': cv_scores.std()
    })
    
    print(f"   Training time: {train_time:.6f} seconds")
    print(f"   Prediction time: {pred_time:.6f} seconds")
    print(f"   Test Accuracy: {accuracy:.4f}")
    print(f"   CV Accuracy: {cv_scores.mean():.4f} (Â±{cv_scores.std():.4f})")

# Create comparison dataframe
results_df = pd.DataFrame(results)
print("\nðŸ“Š ALGORITHM COMPARISON SUMMARY:")
print("-" * 70)
print(results_df.to_string(index=False))

# Find best algorithm
best_algo = results_df.loc[results_df['Accuracy'].idxmax(), 'Algorithm']
best_accuracy = results_df['Accuracy'].max()
print(f"\nðŸ† Best performing algorithm: {best_algo} with accuracy {best_accuracy:.4f}")

# ============================================================
# PART 5: FEATURE IMPORTANCE AND SELECTION
# ============================================================
print("\n" + "=" * 80)
print("ðŸ”¬ PART 5: FEATURE IMPORTANCE ANALYSIS")
print("=" * 80)

# Train final model with best algorithm
knn_final = KNeighborsClassifier(
    n_neighbors=5,
    algorithm='brute' if best_algo == 'Brute Force' else 
              ('kd_tree' if best_algo == 'KD-Tree' else 'ball_tree'),
    metric='euclidean'
)
knn_final.fit(X_train_scaled, y_train)
y_pred_final = knn_final.predict(X_test_scaled)

# Since KNN doesn't provide direct feature importance,
# we'll use a simple permutation importance approach
def permutation_importance(model, X, y, n_repeats=10):
    baseline = accuracy_score(y, model.predict(X))
    importances = []
    
    for col in range(X.shape[1]):
        scores = []
        for _ in range(n_repeats):
            X_permuted = X.copy()
            X_permuted[:, col] = np.random.permutation(X_permuted[:, col])
            score = accuracy_score(y, model.predict(X_permuted))
            scores.append(baseline - score)
        importances.append(np.mean(scores))
    
    return np.array(importances)

# Calculate permutation importance
importances = permutation_importance(knn_final, X_test_scaled, y_test, n_repeats=5)

# Create feature importance dataframe
feature_importance_df = pd.DataFrame({
    'Feature': X_missing.columns,
    'Importance': importances
}).sort_values('Importance', ascending=False)

print("\nðŸ“Š Feature Importance (Permutation-based):")
print("-" * 50)
print(feature_importance_df.to_string(index=False))

# Select top 2 features for decision boundary
top_2_features = feature_importance_df.head(2)['Feature'].values
print(f"\nðŸŽ¯ Top 2 features for decision boundary: {top_2_features[0]}, {top_2_features[1]}")

# Get indices of top features
top_2_indices = [list(X_missing.columns).index(f) for f in top_2_features]

# Create dataset with only top 2 features
X_train_top2 = X_train_scaled[:, top_2_indices]
X_test_top2 = X_test_scaled[:, top_2_indices]

# Train KNN on top 2 features
knn_top2 = KNeighborsClassifier(n_neighbors=5)
knn_top2.fit(X_train_top2, y_train)
y_pred_top2 = knn_top2.predict(X_test_top2)
acc_top2 = accuracy_score(y_test, y_pred_top2)

print(f"\nðŸ“ˆ KNN on top 2 features - Accuracy: {acc_top2:.4f}")
print(f"   (vs {best_accuracy:.4f} with all features)")

# ============================================================
# PART 6: PLOT DECISION BOUNDARY
# ============================================================
print("\n" + "=" * 80)
print("ðŸ”¬ PART 6: PLOTTING DECISION BOUNDARY")
print("=" * 80)

# Create visualizations
fig, axes = plt.subplots(2, 3, figsize=(18, 12))

# Plot 1: Confusion Matrix for best algorithm
ax1 = axes[0, 0]
cm = confusion_matrix(y_test, y_pred_final)
sns.heatmap(cm, annot=True, fmt='d', cmap='Blues', ax=ax1,
            xticklabels=['No Diabetes', 'Diabetes'],
            yticklabels=['No Diabetes', 'Diabetes'])
ax1.set_xlabel('Predicted')
ax1.set_ylabel('Actual')
ax1.set_title(f'Confusion Matrix - {best_algo}\nAccuracy: {best_accuracy:.4f}')

# Plot 2: Algorithm comparison - Time
ax2 = axes[0, 1]
x = np.arange(len(results))
width = 0.35
train_times = [r['Train Time (s)'] for r in results]
predict_times = [r['Predict Time (s)'] for r in results]
ax2.bar(x - width/2, train_times, width, label='Train Time', color='skyblue', edgecolor='black')
ax2.bar(x + width/2, predict_times, width, label='Predict Time', color='lightcoral', edgecolor='black')
ax2.set_xlabel('Algorithm')
ax2.set_ylabel('Time (seconds)')
ax2.set_title('Algorithm Training and Prediction Time')
ax2.set_xticks(x)
ax2.set_xticklabels([r['Algorithm'] for r in results])
ax2.legend()
ax2.grid(True, alpha=0.3, axis='y')

# Plot 3: Algorithm comparison - Accuracy
ax3 = axes[0, 2]
accuracies = [r['Accuracy'] for r in results]
bars = ax3.bar([r['Algorithm'] for r in results], accuracies, 
               color=['skyblue', 'lightcoral', 'lightgreen'], edgecolor='black')
ax3.set_ylabel('Accuracy')
ax3.set_title('Algorithm Accuracy Comparison')
ax3.set_ylim(0.6, 0.8)
ax3.grid(True, alpha=0.3, axis='y')

# Add value labels
for bar, acc in zip(bars, accuracies):
    height = bar.get_height()
    ax3.text(bar.get_x() + bar.get_width()/2., height + 0.005,
             f'{acc:.4f}', ha='center', va='bottom', fontweight='bold')

# Plot 4: Decision Boundary (top 2 features)
ax4 = axes[1, 0]
# Create mesh
x_min, x_max = X_train_top2[:, 0].min() - 1, X_train_top2[:, 0].max() + 1
y_min, y_max = X_train_top2[:, 1].min() - 1, X_train_top2[:, 1].max() + 1
xx, yy = np.meshgrid(np.arange(x_min, x_max, 0.05),
                     np.arange(y_min, y_max, 0.05))

# Predict on mesh
Z = knn_top2.predict(np.c_[xx.ravel(), yy.ravel()])
Z = Z.reshape(xx.shape)

# Plot decision boundary
ax4.contourf(xx, yy, Z, alpha=0.3, cmap='coolwarm')
ax4.scatter(X_train_top2[:, 0], X_train_top2[:, 1], 
           c=y_train, cmap='coolwarm', edgecolor='black', 
           alpha=0.7, s=30)
ax4.set_xlabel(f'{top_2_features[0]} (scaled)')
ax4.set_ylabel(f'{top_2_features[1]} (scaled)')
ax4.set_title(f'Decision Boundary - Top 2 Features\nAccuracy: {acc_top2:.4f}')
ax4.grid(True, alpha=0.3)

# Plot 5: Feature Importance
ax5 = axes[1, 1]
bars = ax5.barh(feature_importance_df['Feature'], feature_importance_df['Importance'],
                color='skyblue', edgecolor='black')
ax5.set_xlabel('Permutation Importance')
ax5.set_title('Feature Importance (Permutation-based)')
ax5.grid(True, alpha=0.3, axis='x')

# Highlight top 2 features
for i, (bar, feat) in enumerate(zip(bars, feature_importance_df['Feature'])):
    if feat in top_2_features:
        bar.set_color('red')
        bar.set_alpha(0.7)

# Plot 6: Classification Report Heatmap
ax6 = axes[1, 2]
report = classification_report(y_test, y_pred_final, output_dict=True)
report_df = pd.DataFrame(report).iloc[:-1, :3]  # Remove support row, keep only first 3 cols
sns.heatmap(report_df, annot=True, fmt='.3f', cmap='Blues', ax=ax6, cbar=False)
ax6.set_title('Classification Report Metrics')

plt.suptitle('KNN Imputation and Algorithm Comparison - Pima Indians Diabetes', fontsize=14, y=1.02)
plt.tight_layout()
plt.show()

# Print final summary
print("\n" + "=" * 80)
print("ðŸ“Œ SUMMARY AND KEY FINDINGS")
print("=" * 80)
print(f"""
âœ… **Key Findings:**

1. **KNN Imputation:** Successfully handled missing values ({X_missing.isnull().sum().sum()} missing values imputed)
   with minimal impact on feature distributions.

2. **Algorithm Comparison:**
   - **Brute Force:** {results_df.iloc[0]['Accuracy']:.4f} accuracy, {results_df.iloc[0]['Train Time (s)']:.6f}s train time
   - **KD-Tree:** {results_df.iloc[1]['Accuracy']:.4f} accuracy, {results_df.iloc[1]['Train Time (s)']:.6f}s train time
   - **Ball Tree:** {results_df.iloc[2]['Accuracy']:.4f} accuracy, {results_df.iloc[2]['Train Time (s)']:.6f}s train time

3. **Best Algorithm:** {best_algo} with {best_accuracy:.4f} accuracy

4. **Most Important Features:**
   - {feature_importance_df.iloc[0]['Feature']}: {feature_importance_df.iloc[0]['Importance']:.4f}
   - {feature_importance_df.iloc[1]['Feature']}: {feature_importance_df.iloc[1]['Importance']:.4f}

5. **Decision Boundary:** Using top 2 features achieves {acc_top2:.4f} accuracy,
   showing reasonable separation between classes.

ðŸ’¡ **Conclusion:** 
   - All three algorithms achieve similar accuracy, with {best_algo} slightly better.
   - Training time differences are minimal for this dataset size.
   - KNN imputation effectively handles missing values without introducing significant bias.
   - The top features align with medical knowledge (Glucose, BMI are known diabetes risk factors).
""")
