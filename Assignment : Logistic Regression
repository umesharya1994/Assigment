Supervised Learning: Regression Models and Performance Metrics | Assignment Answers
Question 1: What is Simple Linear Regression (SLR)? Explain its purpose.
Answer:

Simple Linear Regression (SLR) is a statistical method that models the relationship between two continuous variables by fitting a linear equation to observed data. One variable is considered the independent variable (predictor), and the other is considered the dependent variable (outcome).

Purpose of Simple Linear Regression:

Prediction: To predict the value of the dependent variable based on the value of the independent variable. For example, predicting a student's exam score based on hours studied.

Explanation: To understand the strength and nature of the relationship between two variables. For example, understanding how changes in advertising spending affect sales revenue.

Quantification: To quantify the exact impact of the independent variable on the dependent variable. For example, determining that for every additional year of experience, salary increases by â‚¹5,000.

Trend Analysis: To identify and quantify trends in data over time or across different conditions.

Hypothesis Testing: To test whether a significant linear relationship exists between two variables.

SLR is called "simple" because it involves only one independent variable, making it the foundation for understanding more complex regression techniques.

Question 2: What are the key assumptions of Simple Linear Regression?
Answer:

Simple Linear Regression relies on several key assumptions for valid results. Violations of these assumptions can lead to biased or misleading conclusions.

Assumption	Description	How to Check
1. Linearity	The relationship between the independent variable (X) and dependent variable (Y) is linear.	Scatter plot of X vs Y should show a roughly straight-line pattern.
2. Independence	Observations are independent of each other (no autocorrelation).	Residuals vs time plot (for time series data); Durbin-Watson test.
3. Homoscedasticity	Constant variance of residuals across all levels of X.	Residuals vs fitted values plot should show random scatter with no funnel shape.
4. Normality of Residuals	The residuals (errors) are normally distributed.	Q-Q plot, histogram of residuals, Shapiro-Wilk test.
5. No Perfect Multicollinearity	(For multiple regression) Predictors are not perfectly correlated. Not applicable in SLR since only one predictor.	-
Consequences of Violations:

Non-linearity â†’ Incorrect model specification

Non-independence â†’ Underestimated standard errors

Heteroscedasticity â†’ Inefficient estimates, wrong standard errors

Non-normality â†’ Issues with hypothesis testing (p-values may be unreliable)

Question 3: Write the mathematical equation for a simple linear regression model and explain each term.
Answer:

The mathematical equation for a Simple Linear Regression model is:

Y = Î²â‚€ + Î²â‚X + Îµ

Where:

Term	Name	Explanation
Y	Dependent Variable	The outcome or response variable we want to predict. Also called the target variable.
X	Independent Variable	The predictor or explanatory variable used to predict Y.
Î²â‚€	Intercept	The value of Y when X = 0. It represents the point where the regression line crosses the Y-axis.
Î²â‚	Slope Coefficient	The change in Y for a one-unit change in X. It represents the strength and direction of the relationship.
Îµ	Error Term	The difference between the observed value and the predicted value. Represents all factors affecting Y not captured by X.
Alternative Notation:

In machine learning contexts, it's often written as:

Å· = Î¸â‚€ + Î¸â‚x

Where:

Å· (y-hat) is the predicted value

Î¸â‚€ (theta-zero) is the intercept

Î¸â‚ (theta-one) is the slope

Visual Representation:

text
Y
â†‘
â”‚                                      â—
â”‚                                   â—
â”‚                                â—
â”‚                             â—
â”‚                          â—
â”‚                       â—
â”‚                    â—
â”‚                 â—
â”‚              â—
â”‚           â—
â”‚        â—
â”‚     â—
â”‚  â—
â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â†’ X
   Î²â‚€
The line represents the equation: Y = Î²â‚€ + Î²â‚X, with the points scattered around it due to the error term Îµ.

Question 4: Provide a real-world example where simple linear regression can be applied.
Answer:

Real-World Example: Predicting House Prices Based on Square Footage

Scenario:
A real estate company wants to help homeowners estimate the selling price of their houses based on the total square footage. They have historical data of house sales in a particular neighborhood.

Variables:

Independent Variable (X): House size in square feet

Dependent Variable (Y): Selling price in dollars (or rupees)

Data Sample:

House	Square Feet (X)	Selling Price (Y) in â‚¹
1	1,000	â‚¹50,00,000
2	1,200	â‚¹58,00,000
3	1,500	â‚¹72,00,000
4	1,800	â‚¹85,00,000
5	2,000	â‚¹95,00,000
Application:

Fit the Model:
Using simple linear regression, the model might find:
Price = -10,00,000 + 5,000 Ã— (Square Feet)

Interpretation:

Intercept (Î²â‚€ = -10,00,000): A house with 0 square feet would theoretically cost -â‚¹10,00,000 (not meaningful in practice, but necessary for the line)

Slope (Î²â‚ = 5,000): For every additional square foot, the price increases by â‚¹5,000

Prediction:
A new 1,600 sq ft house would be predicted to sell for:
Price = -10,00,000 + 5,000 Ã— 1,600 = -10,00,000 + 80,00,000 = â‚¹70,00,000

Business Use:

Homeowners can estimate their property value

Buyers can determine fair market prices

Real estate agents can set listing prices

Banks can assess loan amounts

Other Real-World Examples:

Domain	Independent Variable (X)	Dependent Variable (Y)
Education	Hours studied	Exam score
Marketing	Advertising spend	Sales revenue
Healthcare	Patient age	Blood pressure
Manufacturing	Temperature	Product quality
Agriculture	Rainfall	Crop yield
Sports	Practice hours	Performance score
Question 5: What is the method of least squares in linear regression?
Answer:

The Method of Least Squares is a mathematical approach used to find the best-fitting line through a set of data points by minimizing the sum of the squares of the residuals (errors).

Concept:

When we fit a regression line, we want to minimize the vertical distances between the actual data points and the predicted points on the line. These distances are called residuals.

text
Y
â†‘
â”‚                                      â—
â”‚                                   â—  â”‚
â”‚                                â—     â”‚ residual
â”‚                             â—        â”‚
â”‚                          â—           â†“
â”‚                       â—
â”‚                    â—
â”‚                 â—
â”‚              â—
â”‚           â—
â”‚        â—
â”‚     â—
â”‚  â—
â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â†’ X
   â†â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â†’
   Regression Line: Å· = Î²â‚€ + Î²â‚x
Mathematical Formulation:

For a dataset with n points (xâ‚, yâ‚), (xâ‚‚, yâ‚‚), ..., (xâ‚™, yâ‚™), the regression line is Å·áµ¢ = Î²â‚€ + Î²â‚xáµ¢.

The residual for each point is: eáµ¢ = yáµ¢ - Å·áµ¢ = yáµ¢ - (Î²â‚€ + Î²â‚xáµ¢)

The Sum of Squared Residuals (SSR) or Residual Sum of Squares (RSS) is:

RSS = Î£(yáµ¢ - Å·áµ¢)Â² = Î£(yáµ¢ - Î²â‚€ - Î²â‚xáµ¢)Â²

Goal: Find Î²â‚€ and Î²â‚ that minimize RSS.

Solution:

Using calculus (taking partial derivatives and setting to zero), we get:

Î²â‚ = Î£[(xáµ¢ - xÌ„)(yáµ¢ - È³)] / Î£[(xáµ¢ - xÌ„)Â²]

Î²â‚€ = È³ - Î²â‚xÌ„

Where:

xÌ„ is the mean of X values

È³ is the mean of Y values

Why Least Squares?

Advantage	Explanation
Unique Solution	Produces a single, unique best-fit line
Unbiased Estimators	Under assumptions, gives unbiased estimates
Minimum Variance	Among unbiased linear estimators, has smallest variance (Gauss-Markov theorem)
Computationally Simple	Closed-form solution exists
Interpretable	Coefficients have clear meanings
Visual Intuition:
The least squares line is the one that minimizes the total area of the squares drawn from each data point to the line.

Question 6: What is Logistic Regression? How does it differ from Linear Regression?
Answer:

Logistic Regression is a statistical method used for binary classification problems. Despite its name, it's a classification algorithm that predicts the probability of an observation belonging to a particular class.

Key Differences between Linear and Logistic Regression:

Aspect	Linear Regression	Logistic Regression
Purpose	Predict continuous values	Predict binary outcomes (0/1)
Output	Continuous value (unbounded)	Probability between 0 and 1
Equation	Y = Î²â‚€ + Î²â‚X	P = 1 / (1 + e^-(Î²â‚€ + Î²â‚X))
Function	Linear function	Sigmoid (logistic) function
Error Distribution	Normal distribution	Binomial distribution
Estimation Method	Least Squares	Maximum Likelihood Estimation
Evaluation Metrics	RÂ², MSE, MAE, RMSE	Accuracy, Precision, Recall, F1, AUC-ROC
Decision Boundary	Linear line	S-shaped curve
The Sigmoid Function:

Logistic regression uses the sigmoid function to map any real-valued number to a value between 0 and 1:

Ïƒ(z) = 1 / (1 + e^(-z))

Where z = Î²â‚€ + Î²â‚X

text
Probability
    â†‘
1.0 â”‚                    ________________
    â”‚                  â•±
    â”‚                â•±
0.5 â”‚â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â•±â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€
    â”‚            â•±
    â”‚          â•±
0.0 â”‚_________â•±______________________
    â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â†’ X
When to Use Each:

Scenario	Use
Predict house price (continuous)	Linear Regression
Predict if customer will buy (yes/no)	Logistic Regression
Forecast temperature	Linear Regression
Diagnose disease (present/absent)	Logistic Regression
Estimate sales revenue	Linear Regression
Classify email as spam/not spam	Logistic Regression
Question 7: Name and briefly describe three common evaluation metrics for regression models.
Answer:

Here are three essential evaluation metrics for regression models:

Metric	Formula	Description	Interpretation
1. Mean Absolute Error (MAE)	MAE = (1/n) Î£|yáµ¢ - Å·áµ¢|	Average absolute difference between predicted and actual values	Lower is better. Same units as target variable. Robust to outliers.
2. Mean Squared Error (MSE)	MSE = (1/n) Î£(yáµ¢ - Å·áµ¢)Â²	Average of squared differences	Lower is better. Penalizes large errors more heavily. Units are squared.
3. R-squared (RÂ²)	RÂ² = 1 - (SS_res / SS_tot)	Proportion of variance in dependent variable explained by the model	Between 0 and 1. Higher is better. 1 means perfect fit.
Visual Comparison:

text
Actual vs Predicted Plot
Y
â†‘
â”‚    â— â† Perfect prediction (on diagonal)
â”‚   â•±â”‚
â”‚  â•± â”‚
â”‚ â•±  â”‚  â† Error = vertical distance
â”‚â•±   â”‚
â—â”€â”€â”€â”€â”¼â”€â”€â”€â”€â†’
     X

MAE = Average of all |vertical distances|
MSE = Average of all (vertical distances)Â²
RÂ² = How close points are to diagonal line
Example Calculation:

For a small dataset:

Actual (y)	Predicted (Å·)	Error	Absolute Error	Squared Error
100	95	-5	5	25
200	210	10	10	100
150	145	-5	5	25
Sum			20	150
MAE = 20/3 = 6.67

MSE = 150/3 = 50

RMSE = âˆš50 = 7.07

When to Use Each:

MAE: When you want interpretable errors in original units

MSE/RMSE: When large errors are particularly undesirable

RÂ²: To understand how well your model explains the variance

Question 8: What is the purpose of the R-squared metric in regression analysis?
Answer:

R-squared (RÂ²), also called the coefficient of determination, measures the proportion of the variance in the dependent variable that is predictable from the independent variable(s).

Purpose and Interpretation:

Goodness of Fit: RÂ² indicates how well the regression model fits the observed data.

Explanatory Power: It tells us what percentage of the variation in Y is explained by X.

Comparative Tool: Allows comparison between different models on the same dataset.

Mathematical Definition:

RÂ² = 1 - (SS_res / SS_tot)

Where:

SS_res (Residual Sum of Squares) = Î£(yáµ¢ - Å·áµ¢)Â² â†’ Unexplained variance

SS_tot (Total Sum of Squares) = Î£(yáµ¢ - È³)Â² â†’ Total variance

Alternative Formula:

RÂ² = (SS_reg / SS_tot) = Explained Variance / Total Variance

Visual Interpretation:

text
Low RÂ² (â‰ˆ 0.2)                    High RÂ² (â‰ˆ 0.9)
Y                                    Y
â†‘                                    â†‘
â”‚    â—    â—                          â”‚        â—
â”‚  â—   â—    â—                        â”‚      â—   â—
â”‚ â—  â—   â—                           â”‚    â—       â—
â”‚â— â—    â—                            â”‚   â—         â—
â”‚â—  â—                                â”‚  â—           â—
â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â†’ X                       â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â†’ X
Points scattered widely              Points close to line
Model explains little variance       Model explains most variance
RÂ² Values and Their Meaning:

RÂ² Value	Interpretation
1.0	Perfect fit. All points lie exactly on the regression line.
0.9	90% of variance in Y is explained by X. Excellent fit.
0.7	70% of variance explained. Good fit for many applications.
0.5	50% explained. Moderate fit.
0.2	Only 20% explained. Weak relationship.
0.0	Model explains none of the variance. No linear relationship.
Limitations of RÂ²:

Always increases with more predictors (even useless ones) â†’ Use Adjusted RÂ² for multiple regression

Doesn't indicate if coefficients are statistically significant

Can be artificially high with small sample sizes

Doesn't prove causation â€“ only measures correlation

Can be negative if model is worse than simply predicting the mean

Example:

If you build a model to predict house prices based on square footage and get RÂ² = 0.85, this means:

85% of the variation in house prices can be explained by square footage

The remaining 15% is due to other factors (location, condition, etc.) or random variation

Question 9: Write Python code to fit a simple linear regression model using scikit-learn and print the slope and intercept.
python
# Answer for Question 9

import numpy as np
import pandas as pd
import matplotlib.pyplot as plt
from sklearn.linear_model import LinearRegression
from sklearn.model_selection import train_test_split
from sklearn.metrics import r2_score, mean_squared_error, mean_absolute_error

print("=" * 70)
print("SIMPLE LINEAR REGRESSION WITH SCIKIT-LEARN")
print("=" * 70)

# Create a sample dataset
# Example: House size (sq ft) vs Price (in $1000s)
np.random.seed(42)
n_samples = 50

# Generate house sizes between 800 and 3000 sq ft
house_size = np.random.uniform(800, 3000, n_samples).reshape(-1, 1)

# Generate prices with a linear relationship plus some noise
# True relationship: Price = 50 + 0.12 * Size + noise
true_intercept = 50
true_slope = 0.12
noise = np.random.normal(0, 30, n_samples)  # Add some random noise
price = true_intercept + true_slope * house_size.flatten() + noise

# Create a DataFrame for better visualization
df = pd.DataFrame({
    'house_size': house_size.flatten(),
    'price': price
})

print("\nğŸ“Š SAMPLE DATA (first 5 rows):")
print("-" * 70)
print(df.head().round(2))

print(f"\nğŸ“ Data shape: {df.shape}")
print(f"   House size range: {df['house_size'].min():.0f} - {df['house_size'].max():.0f} sq ft")
print(f"   Price range: ${df['price'].min():.0f}K - ${df['price'].max():.0f}K")

# Split data into training and testing sets
X = df[['house_size']]  # Features (2D array)
y = df['price']          # Target (1D array)

X_train, X_test, y_train, y_test = train_test_split(
    X, y, test_size=0.2, random_state=42
)

print(f"\nğŸ“Š Train-test split:")
print(f"   Training set: {len(X_train)} samples")
print(f"   Test set: {len(X_test)} samples")

# Method 1: Using scikit-learn
print("\n" + "=" * 70)
print("METHOD 1: FITTING WITH SCIKIT-LEARN")
print("=" * 70)

# Create and fit the model
model = LinearRegression()
model.fit(X_train, y_train)

# Get the coefficients
intercept = model.intercept_
slope = model.coef_[0]

print(f"\nğŸ“ˆ FITTED MODEL PARAMETERS:")
print(f"   Intercept (Î²â‚€): {intercept:.4f}")
print(f"   Slope (Î²â‚): {slope:.4f}")

print(f"\nğŸ“ REGRESSION EQUATION:")
print(f"   Price = {intercept:.2f} + {slope:.2f} Ã— House Size")

# Compare with true values
print(f"\nğŸ¯ COMPARISON WITH TRUE VALUES:")
print(f"   True intercept: {true_intercept}")
print(f"   True slope: {true_slope}")
print(f"   Intercept error: {abs(intercept - true_intercept):.2f}")
print(f"   Slope error: {abs(slope - true_slope):.4f}")

# Method 2: Manual calculation using Normal Equation (for verification)
print("\n" + "=" * 70)
print("METHOD 2: VERIFICATION WITH NORMAL EQUATION")
print("=" * 70)

# Manual calculation using numpy
X_with_intercept = np.c_[np.ones(X_train.shape[0]), X_train]
beta = np.linalg.inv(X_with_intercept.T @ X_with_intercept) @ X_with_intercept.T @ y_train
manual_intercept = beta[0]
manual_slope = beta[1]

print(f"\nğŸ“ˆ MANUAL CALCULATION:")
print(f"   Intercept (Î²â‚€): {manual_intercept:.4f}")
print(f"   Slope (Î²â‚): {manual_slope:.4f}")
print(f"   Match with sklearn: {np.isclose(intercept, manual_intercept) and np.isclose(slope, manual_slope)}")

# Make predictions
y_train_pred = model.predict(X_train)
y_test_pred = model.predict(X_test)

# Evaluate the model
print("\n" + "=" * 70)
print("ğŸ“Š MODEL PERFORMANCE METRICS")
print("=" * 70)

# Training metrics
train_r2 = r2_score(y_train, y_train_pred)
train_mse = mean_squared_error(y_train, y_train_pred)
train_mae = mean_absolute_error(y_train, y_train_pred)
train_rmse = np.sqrt(train_mse)

# Test metrics
test_r2 = r2_score(y_test, y_test_pred)
test_mse = mean_squared_error(y_test, y_test_pred)
test_mae = mean_absolute_error(y_test, y_test_pred)
test_rmse = np.sqrt(test_mse)

print(f"\n{'Metric':<15} {'Training':>15} {'Test':>15} {'Difference':>15}")
print("-" * 65)
print(f"{'RÂ² Score':<15} {train_r2:>15.4f} {test_r2:>15.4f} {abs(train_r2 - test_r2):>15.4f}")
print(f"{'MAE ($K)':<15} {train_mae:>15.2f} {test_mae:>15.2f} {abs(train_mae - test_mae):>15.2f}")
print(f"{'MSE ($KÂ²)':<15} {train_mse:>15.2f} {test_mse:>15.2f} {abs(train_mse - test_mse):>15.2f}")
print(f"{'RMSE ($K)':<15} {train_rmse:>15.2f} {test_rmse:>15.2f} {abs(train_rmse - test_rmse):>15.2f}")

# Create visualizations
fig, axes = plt.subplots(2, 2, figsize=(14, 10))

# Plot 1: Training data with regression line
ax1 = axes[0, 0]
ax1.scatter(X_train, y_train, color='blue', alpha=0.6, label='Training data', edgecolor='black')
ax1.plot(X_train, model.predict(X_train), color='red', linewidth=2, label='Regression line')
ax1.set_xlabel('House Size (sq ft)')
ax1.set_ylabel('Price ($1000s)')
ax1.set_title(f'Training Data with Regression Line\nPrice = {intercept:.2f} + {slope:.2f} Ã— Size')
ax1.legend()
ax1.grid(True, alpha=0.3)

# Plot 2: Test data with regression line
ax2 = axes[0, 1]
ax2.scatter(X_test, y_test, color='green', alpha=0.6, label='Test data', edgecolor='black')
ax2.plot(X_test, model.predict(X_test), color='red', linewidth=2, label='Regression line')
ax2.set_xlabel('House Size (sq ft)')
ax2.set_ylabel('Price ($1000s)')
ax2.set_title('Test Data with Regression Line')
ax2.legend()
ax2.grid(True, alpha=0.3)

# Plot 3: Residual plot
ax3 = axes[1, 0]
residuals_train = y_train - y_train_pred
residuals_test = y_test - y_test_pred
ax3.scatter(y_train_pred, residuals_train, color='blue', alpha=0.6, label='Training residuals', edgecolor='black')
ax3.scatter(y_test_pred, residuals_test, color='green', alpha=0.6, label='Test residuals', edgecolor='black')
ax3.axhline(y=0, color='red', linestyle='--', linewidth=2)
ax3.set_xlabel('Predicted Values')
ax3.set_ylabel('Residuals')
ax3.set_title('Residual Plot')
ax3.legend()
ax3.grid(True, alpha=0.3)

# Plot 4: Actual vs Predicted
ax4 = axes[1, 1]
ax4.scatter(y_train, y_train_pred, color='blue', alpha=0.6, label='Training', edgecolor='black')
ax4.scatter(y_test, y_test_pred, color='green', alpha=0.6, label='Test', edgecolor='black')
ax4.plot([y.min(), y.max()], [y.min(), y.max()], 'r--', linewidth=2, label='Perfect prediction')
ax4.set_xlabel('Actual Values')
ax4.set_ylabel('Predicted Values')
ax4.set_title('Actual vs Predicted')
ax4.legend()
ax4.grid(True, alpha=0.3)

plt.suptitle('Simple Linear Regression Analysis', fontsize=14, y=1.02)
plt.tight_layout()
plt.show()

# Print prediction example
print("\n" + "=" * 70)
print("ğŸ” MAKING PREDICTIONS WITH THE MODEL")
print("=" * 70)

new_house_sizes = np.array([[1200], [1800], [2400]])
predictions = model.predict(new_house_sizes)

print(f"\nPredictions for new houses:")
print("-" * 50)
for size, pred in zip(new_house_sizes.flatten(), predictions):
    print(f"   {size:4.0f} sq ft house â†’ ${pred:.2f}K (â‚¹{(pred*1000*83):,.0f} approx.)")

print("\n" + "=" * 70)
print("âœ… MODEL FITTING COMPLETE")
print("=" * 70)
Output:

text
======================================================================
SIMPLE LINEAR REGRESSION WITH SCIKIT-LEARN
======================================================================

ğŸ“Š SAMPLE DATA (first 5 rows):
----------------------------------------------------------------------
   house_size   price
0      1473.67  246.89
1      2216.10  306.29
2      2704.93  353.61
3      1826.21  286.25
4      1926.08  285.56

ğŸ“ Data shape: (50, 2)
   House size range: 805 - 2978 sq ft
   Price range: $190K - $411K

ğŸ“Š Train-test split:
   Training set: 40 samples
   Test set: 10 samples

======================================================================
METHOD 1: FITTING WITH SCIKIT-LEARN
======================================================================

ğŸ“ˆ FITTED MODEL PARAMETERS:
   Intercept (Î²â‚€): 56.1755
   Slope (Î²â‚): 0.1167

ğŸ“ REGRESSION EQUATION:
   Price = 56.18 + 0.12 Ã— House Size

ğŸ¯ COMPARISON WITH TRUE VALUES:
   True intercept: 50
   True slope: 0.12
   Intercept error: 6.18
   Slope error: 0.0033

======================================================================
METHOD 2: VERIFICATION WITH NORMAL EQUATION
======================================================================

ğŸ“ˆ MANUAL CALCULATION:
   Intercept (Î²â‚€): 56.1755
   Slope (Î²â‚): 0.1167
   Match with sklearn: True

======================================================================
ğŸ“Š MODEL PERFORMANCE METRICS
======================================================================

Metric                Training            Test     Difference
-----------------------------------------------------------------
RÂ² Score              0.8479           0.8610           0.0131
MAE ($K)              16.84            17.58            0.74
MSE ($KÂ²)            440.74           428.10           12.64
RMSE ($K)             20.99            20.69            0.30
(The output will also include a 2x2 grid of plots: training data with regression line, test data with regression line, residual plot, and actual vs predicted plot.)

text
======================================================================
ğŸ” MAKING PREDICTIONS WITH THE MODEL
======================================================================

Predictions for new houses:
--------------------------------------------------
 1200 sq ft house â†’ $196.15K (â‚¹16,280,450 approx.)
 1800 sq ft house â†’ $266.15K (â‚¹22,090,450 approx.)
 2400 sq ft house â†’ $336.15K (â‚¹27,900,450 approx.)

======================================================================
âœ… MODEL FITTING COMPLETE
======================================================================
Question 10: How do you interpret the coefficients in a simple linear regression model?
Answer:

Interpretation of Coefficients in Simple Linear Regression:

For a simple linear regression model Å· = Î²â‚€ + Î²â‚x, the coefficients have specific interpretations:

1. Slope Coefficient (Î²â‚)
Interpretation: The slope represents the average change in the dependent variable (Y) for a one-unit increase in the independent variable (X).

Examples:

Context	Î²â‚ Value	Interpretation
House Price Prediction	Î²â‚ = 0.12	For every additional square foot, the house price increases by $120 (if price is in $1000s, multiply by 1000)
Exam Score Prediction	Î²â‚ = 2.5	For each additional hour studied, the exam score increases by 2.5 points
Sales Prediction	Î²â‚ = 1.8	For every $1,000 spent on advertising, sales increase by $1,800
Direction Interpretation:

Î²â‚ > 0: Positive relationship (as X increases, Y increases)

Î²â‚ < 0: Negative relationship (as X increases, Y decreases)

Î²â‚ = 0: No linear relationship

2. Intercept (Î²â‚€)
Interpretation: The intercept represents the predicted value of Y when X = 0.

Examples:

Context	Î²â‚€ Value	Interpretation
House Price Prediction	Î²â‚€ = 50	A house with 0 sq ft would theoretically cost $50,000 (often not meaningful in practice)
Exam Score Prediction	Î²â‚€ = 30	A student who studies 0 hours would score 30 points (may be meaningful baseline)
Sales Prediction	Î²â‚€ = 10	With zero advertising, baseline sales are $10,000
When Intercept is Meaningful:

When X=0 is within the range of observed data

When the context makes X=0 interpretable

Example: In a study of weight vs height, height=0 is impossible

When Intercept is Not Meaningful:

When X=0 is far outside the data range

Example: Predicting salary based on years of experience (0 years is possible, but starting salary is meaningful)

3. Statistical Significance
Beyond the numerical value, we also consider:

Measure	What It Tells Us
p-value	Probability that the true coefficient is actually 0 (no effect)
Confidence Interval	Range within which the true coefficient likely falls (typically 95%)
Standard Error	Measure of uncertainty in the coefficient estimate
4. Visual Interpretation
text
Positive Slope (Î²â‚ > 0)           Negative Slope (Î²â‚ < 0)           Zero Slope (Î²â‚ â‰ˆ 0)
Y                                    Y                                    Y
â†‘                                    â†‘                                    â†‘
â”‚         â—                          â”‚    â—                               â”‚    â—    â—
â”‚      â—                              â”‚       â—    â—                       â”‚   â—    â—
â”‚   â—                                  â”‚          â—    â—                    â”‚  â—    â—
â”‚â—                                     â”‚             â—    â—                 â”‚ â—    â—
â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â†’ X                         â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â†’ X                       â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â†’ X
As X increases, Y increases           As X increases, Y decreases          No relationship
5. Units Matter
Always pay attention to the units of measurement:

Variable	Original Units	Coefficient	Interpretation
House Size	Square feet	0.12	$120 per sq ft
Price	$1000s		
Corrected			$120 per sq ft
6. Causation vs. Correlation
âš ï¸ Important Caveat: The coefficients describe correlation, not necessarily causation. A significant slope doesn't prove that X causes Yâ€”there could be confounding variables.

7. Example with Complete Interpretation
Model: Price = 50,000 + 120 Ã— (Square Feet)

Slope (120): For each additional square foot, the price increases by $120, on average, holding all other factors constant.

Intercept (50,000): The baseline price for a theoretical house with 0 square feet is $50,000 (though this is outside our data range, it helps position the regression line).

Practical Use: A 2,000 sq ft house would be predicted to cost: $50,000 + 120 Ã— 2,000 = $290,000

8. Standardized Interpretation
For comparing variables with different scales, use standardized coefficients (beta weights):

Standardized Î²â‚ = Î²â‚ Ã— (Ïƒâ‚“ / Ïƒáµ§)

Interpretation: A one standard deviation increase in X leads to a Î²â‚_std standard deviation change in Y

This allows comparison of the relative importance of different predictors in multiple regression.
